{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Résumé parsing and information extraction\n",
    "\n",
    "\n",
    "### CoNVO\n",
    "\n",
    "**Context:** Bloc is a career services management platform that builds smart career and data management tools for job-seekers and the organizations serving them. In particular, Bloc seeks to provide and facilitate access to tools for effectively presenting job-seekers' credentials and matching employers' job postings, and thereby improve outcomes.\n",
    "\n",
    "**Need:** Many job-seekers come to Bloc's platform with a résumé already written. Forcing new users to re-enter all that information before they can utilize other tools (e.g. for résumé evaluation) is tedious, at best. This can also reduce time pressure during in-person sessions facilitated by Bloc, where every second counts.\n",
    "\n",
    "**Vision:** Automated extraction of key information from existing résumés, submitted as PDFs while onboarding new users, in order to facilitate and streamline the process.\n",
    "\n",
    "**Outcome:** A standalone, proof-of-concept process for extracting key résumé information and returning it as structured data, complete with unit tests and documentation on expected usage, limitations, and potential improvements.\n",
    "\n",
    "\n",
    "### Data Summary\n",
    "\n",
    "Bloc has provided ~125 résumés with a variety of styles, layouts, and contents, in PDF format. (Bonus: ~2400 résumés scraped from external sources...) Data quality seems good, and appears to be composed entirely of electronically-generated PDFs rather than (much more troublesome) scans of physical documents.\n",
    "\n",
    "They typically include personal contact information, professional experience, education, and skills; they sometimes include information on other relevant experience (volunteering, leadership, side projects), professional and academic associations, honors and awards, and personal interests; they rarely include a professional objective / statement of purpose and references.\n",
    "\n",
    "Since the amount of data is relatively small, and since résumés are so structured and standardized in terms of the information they include, a rules-based approach seems likely to succeed.\n",
    "\n",
    "\n",
    "### Proposed Methodology\n",
    "\n",
    "Cleanly extracting text from PDFs is tricky, since the format alters or throws out information for the sake of human-friendly layout, formatting, and such. Given this, it's best to use well-established tools for the text extraction, and highly accommodating parsing logic for the texts themselves. Rather than going full-bore on a complex, computer-vision based résumé parsing system, it'll be best to start with more foundational tools of text processing: regular expressions, fuzzy string matching, gazetteers/dictionaries, data sanitization/cleanup, and lots of trial-and-error.\n",
    "\n",
    "See the code below for something to get you started.\n",
    "\n",
    "\n",
    "### Definitions of Success\n",
    "\n",
    "- **Baseline:** A function that accepts a résumé (TBD: as filepath or already-extracted text) and returns structured data for the most common résumé components: contact information, professional experience, education, and skills. The quality of the extracted values may be messy or not fully parsed, but shouldn't contain values for other components. Atypical résumé components may be skipped. This function should have basic unit tests and documentation.\n",
    "- **Target:** A function that accepts a résumé and returns structured data for the most common résumé components (see Baseline), as well as other relevant experience, professional/academic associations, honors and awards, and personal interests. The quality of the extracted values should be almost fully parsed (e.g. no large blocks of relevant but unstructured text) and should not contain values for other components. Atypical résumé components may be skipped. This function should have unit tests covering a variety of expected scenarios and good documentation.\n",
    "- **Stretch:** A function that accepts a résumé and returns structured data for any component that could be reasonably expected in such a document. The quality of the extracted values should be almost fully parsed (see Target). Particularly unusual résumé components may be skipped. This function should have comprehensive unit tests and documentation.\n",
    "\n",
    "Note: We should try to get Bloc's buy-in / feedback on a schema, since they already ingest and store some of this data in their systems.\n",
    "\n",
    "\n",
    "### Risks\n",
    "\n",
    "It's possible that the information included in / extracted from résumés is too complex or varied for sufficiently accurate rules-based parsing, in which case a more sophisticated (ML- or DL-based) approach would be necessary, albeit impractical owing to time and data constraints. It's also possible that a rules-based approach is feasible, but too difficult / large a task for a single day's work.\n",
    "\n",
    "Another separate risk deals with personally-identifiable information (PII), which is intrinsic to a résumé, but which DataKind typically prefers to strip out of the data assigned to volunteers. A relatively practical solution would entail extracting text from the PDFs beforehand, then replacing direct PII (name and contact info) with placeholder values, but we'd still have volunteers working with indirect PII such as education / employment history. DataKind may not be able to abide such a middle ground."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Generation\n",
    "\n",
    "**Note:** Don't run this section! (Besides, you _can't_, because the raw data has not been made available to you.) This is just to help you understand the data's provenance. Instead, download the already-generated datasets from OneDrive. (There's a link on the Bloc Project Home document in Dropbox.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "from faker import Faker\n",
    "\n",
    "import msvdd_bloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPython 3.7.4\n",
      "IPython 7.8.0\n"
     ]
    }
   ],
   "source": [
    "%watermark -v -iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAKER = Faker(local=\"en_US\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dirpath = pathlib.Path(\"/Users/burtondewilde/Desktop/datakind/bloc/msvdd_Bloc/data/resumes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bloc Fellows' résumés\n",
    "\n",
    "**Note:** Programmatically extracting text from a PDF with an atypical layout — such as a résumé — is _tricky_. Mistakes happen, and the results aren't always consistent with how a human would type it out.\n",
    "\n",
    "I tried several options... The Python binding to Apache Tika (`python-tika`) seemed to give the nicest text extractions, although the JVM dependency is unfortunate. `textract` provides a convenient and consistent interface, but results are mediocre and installation involves a lot of extra packages. `pdfminer` and its many forks are highly customizable, but confusing to use and, to be honest, a hot mess as far as code quality goes. I'm surprised Python doesn't have a better solution to this problem, but _whatchagonnado_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# files: 128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/Users/burtondewilde/Desktop/datakind/bloc/raw_data/resumes/fellows/2018FellowsResumes[002-002].pdf',\n",
       " '/Users/burtondewilde/Desktop/datakind/bloc/raw_data/resumes/fellows/2018FellowsResumes[003-003].pdf',\n",
       " '/Users/burtondewilde/Desktop/datakind/bloc/raw_data/resumes/fellows/2018FellowsResumes[004-004].pdf']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_fellows_dirpath = pathlib.Path(\"/Users/burtondewilde/Desktop/datakind/bloc/raw_data/resumes/fellows\")\n",
    "filepaths = msvdd_bloc.data.fileio.get_filepaths(in_fellows_dirpath, \".pdf\")\n",
    "print(\"# files:\", len(filepaths))\n",
    "filepaths[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unable to extract text from /Users/burtondewilde/Desktop/datakind/bloc/raw_data/resumes/fellows/2018FellowsResumes[024-024].pdf\n",
      "unable to extract text from /Users/burtondewilde/Desktop/datakind/bloc/raw_data/resumes/fellows/2018FellowsResumes[027-027].pdf\n"
     ]
    }
   ],
   "source": [
    "text_files = []\n",
    "for i, filepath in enumerate(filepaths):\n",
    "    text = msvdd_bloc.data.resumes.extract_text_from_pdf(filepath, min_len=150)\n",
    "    if not text:\n",
    "        print(\"unable to extract text from\", filepath)\n",
    "        continue\n",
    "    text = msvdd_bloc.data.resumes.clean_fellows_text(text)\n",
    "    text = msvdd_bloc.data.resumes.replace_pii(text, faker=FAKER)\n",
    "    fname = \"fellows_resume_{}.txt\".format(i)\n",
    "    text_files.append((fname, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_fellows_fpath = out_dirpath.joinpath(\"fellows_resumes.zip\")\n",
    "msvdd_bloc.data.fileio.save_text_files_to_zip(out_fellows_fpath, text_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Amina's bonus scraped résumés\n",
    "\n",
    "**Note:** Still waiting on details from Amina..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# files: 2432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/Users/burtondewilde/Desktop/datakind/bloc/raw_data/resumes/bonus/_resume_ab0iv8_hopewell-fluorescence-stainless-steel-haledon-nj.txt',\n",
       " '/Users/burtondewilde/Desktop/datakind/bloc/raw_data/resumes/bonus/_resume_ab2abk_available-upon-request-givers-cmo-icd-new-york-ny.txt',\n",
       " '/Users/burtondewilde/Desktop/datakind/bloc/raw_data/resumes/bonus/_resume_ab3i5x_july-2013-5th-pharm-upon-request-harrison-nj-07029.txt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_bonus_dirpath = pathlib.Path(\"/Users/burtondewilde/Desktop/datakind/bloc/raw_data/resumes/bonus\")\n",
    "filepaths = msvdd_bloc.data.fileio.get_filepaths(in_bonus_dirpath, \".txt\")\n",
    "print(\"# files:\", len(filepaths))\n",
    "filepaths[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_files = []\n",
    "for i, filepath in enumerate(filepaths):\n",
    "    text = msvdd_bloc.data.resumes.extract_text_from_pdf(filepath, min_len=150)\n",
    "    if not text:\n",
    "        print(\"unable to extract text from\", filepath)\n",
    "        continue\n",
    "    text = msvdd_bloc.data.resumes.clean_bonus_text(text)\n",
    "    text = msvdd_bloc.data.resumes.replace_pii(text, faker=FAKER)\n",
    "    fname = \"bonus_resume_{}.txt\".format(i)\n",
    "    text_files.append((fname, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_bonus_fpath = out_dirpath.joinpath(\"bonus_resumes.zip\")\n",
    "msvdd_bloc.data.fileio.save_text_files_to_zip(out_bonus_fpath, text_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import io\n",
    "import operator\n",
    "import pathlib\n",
    "import re\n",
    "\n",
    "from toolz import itertoolz\n",
    "\n",
    "import msvdd_bloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%watermark` not found.\n"
     ]
    }
   ],
   "source": [
    "%watermark -v -iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_BULLETS = re.compile(r\"[\\u25cf\\u2022\\u2023\\u2043]\", flags=re.UNICODE)\n",
    "RE_BREAKING_SPACE = re.compile(r\"(\\r\\n|[\\n\\v]){2,}\", flags=re.UNICODE)\n",
    "RE_NONBREAKING_SPACE = re.compile(r\"[^\\S\\n\\v]+\", flags=re.UNICODE)\n",
    "RE_MONTH = re.compile(\n",
    "    r\"(jan|january|feb|february|mar|march|apr|april|may|jun|june|jul|july|aug|august|sep|september|oct|october|nov|november|dec|december)\",\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "RE_YEAR = re.compile(r\"((19|20)\\d{2})\")\n",
    "\n",
    "SECTION_HEADERS = {\n",
    "    \"education\": {\n",
    "        \"education\",\n",
    "    },\n",
    "    \"experience\": {\n",
    "        \"experience\",\n",
    "        \"work experience\",\n",
    "        \"professional experience\",\n",
    "        \"work & research experience\",\n",
    "        \"relevant experience\",\n",
    "        \"experiences\",\n",
    "        \"additional experience\",\n",
    "        \"leadership\",\n",
    "        \"leadership experience\",\n",
    "        \"leadership and service\",\n",
    "    },\n",
    "    \"skills\": {\n",
    "        \"skills\",\n",
    "        \"technical skills\",\n",
    "        \"skills & expertise\",\n",
    "        \"technological skills\",\n",
    "        \"tools\",\n",
    "        \"languages\",\n",
    "        \"programming languages\",\n",
    "        \"languages and technologies\",\n",
    "        \"language and technologies\",\n",
    "    },\n",
    "    \"achievements\": {\n",
    "        \"achievements\",\n",
    "        \"awards\",\n",
    "        \"honors\",\n",
    "        \"honors & awards\",\n",
    "        \"honors, awards, and memberships\",\n",
    "        \"fellowships & awards\",\n",
    "        \"awards and certifications\",\n",
    "    },\n",
    "    \"projects\": {\n",
    "        \"projects\",\n",
    "        \"side projects\",\n",
    "        \"technical projects\",\n",
    "        \"programming projects\",\n",
    "        \"github projects\",\n",
    "        \"other projects\",\n",
    "    },\n",
    "    \"activities\": {\n",
    "        \"activities\",\n",
    "        \"volunteering\",\n",
    "        \"activities and student groups\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_resume_text(text):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        text (str)\n",
    "        \n",
    "    Returns:\n",
    "        str\n",
    "    \"\"\"\n",
    "    # clean up weird stuff\n",
    "    text = RE_BULLETS.sub(\"-\", text)\n",
    "    # normalize whitespace\n",
    "    text = RE_NONBREAKING_SPACE.sub(\" \", text).strip()\n",
    "    text = RE_BREAKING_SPACE.sub(r\"\\n\\n\", text)\n",
    "    # TODO: any other roughness that can be consistently smoothed out\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_section_idxs(lines):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        lines (List[str])\n",
    "    \n",
    "    Returns:\n",
    "        List[Tuple[str, int]]\n",
    "    \"\"\"\n",
    "    section_idxs = [(\"START\", 0)]\n",
    "    for idx, line in enumerate(lines):\n",
    "        for section, headers in SECTION_HEADERS.items():\n",
    "            if (\n",
    "                any(line.lower() == header for header in headers) or\n",
    "                any(line.lower().startswith(header + \":\") for header in headers)\n",
    "            ):\n",
    "                section_idxs.append((section, idx))\n",
    "    section_idxs.append((\"END\", len(lines)))\n",
    "    return section_idxs\n",
    "\n",
    "\n",
    "def get_section_lines(lines, section_idxs):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        lines (List[str])\n",
    "        section_idxs (List[Tuple[str, int]])\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, List[str]]\n",
    "    \"\"\"\n",
    "    section_lines = collections.defaultdict(list)\n",
    "    for (section, idx1), (_, idx2) in itertoolz.sliding_window(2, section_idxs):\n",
    "        section_lines[section].extend(lines[idx1 : idx2])\n",
    "    return dict(section_lines)\n",
    "\n",
    "\n",
    "def parse_skills_section(lines):\n",
    "    \"\"\"\n",
    "    Super rough example for extracting structured data from skills...\n",
    "    \n",
    "    Args:\n",
    "        lines (List[str])\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, List[str]]\n",
    "    \"\"\"\n",
    "    skills = [\n",
    "        skill.lstrip(\"- \")\n",
    "        for line in lines\n",
    "        for skill in re.split(r\", +\", line)\n",
    "        if skill.strip() and\n",
    "        skill.strip().lower() not in SECTION_HEADERS[\"skills\"]\n",
    "    ]\n",
    "    return {\"skills\": skills}\n",
    "\n",
    "\n",
    "def parse_education_section(lines):\n",
    "    \"\"\"\n",
    "    Super rough example for extracting structured data from education...\n",
    "    \n",
    "    Args:\n",
    "        lines (List[str])\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, List[str]]\n",
    "    \"\"\"\n",
    "    \n",
    "    school = [line for line in lines if \"University\" in line or \"university\" in line][0]\n",
    "    graduation = ''\n",
    "    degree = ''\n",
    "    coursework = []\n",
    "    for line in lines:\n",
    "        graduationSearch = re.search( r'.*Graduation: (.*)', line)\n",
    "        if graduationSearch:\n",
    "            graduation = graduationSearch.group(1)\n",
    "            break\n",
    "        degreeSearch = re.search( r')\n",
    "        courses = re.search( r'coursework: )\n",
    "        \n",
    "\n",
    "    return {\"school\": school,\n",
    "            \"graduationDate\": graduation,\n",
    "            \"coursework\": coursework}\n",
    "    \n",
    "\n",
    "# and so on and so forth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumes_fpath =\"/Users/anluc/source/repos/DataKind/msvdd_Bloc/data/resumes/fellows_resumes.zip\"\n",
    "for fname, text in msvdd_bloc.data.fileio.load_text_files_from_zip(resumes_fpath):\n",
    "    text = preprocess_resume_text(text)\n",
    "    lines = [line.strip() for line in text.split(\"\\n\")]\n",
    "    section_idxs = get_section_idxs(lines)\n",
    "    section_lines = get_section_lines(lines, section_idxs)\n",
    "    break  # just stopping here so we can test things out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('START', 0),\n",
       " ('experience', 3),\n",
       " ('experience', 31),\n",
       " ('education', 44),\n",
       " ('projects', 53),\n",
       " ('skills', 58),\n",
       " ('END', 65)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "section_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EDUCATION',\n",
       " 'University of Minnesota - Twin Cities',\n",
       " 'Bachelor of Science Computer Science \\u200bAnticipated Graduation: May 2020',\n",
       " 'Relevant coursework:',\n",
       " '',\n",
       " '- Introduction to C/C++ Programming for Scientists and Engineers; Introduction to Algorithms, Data',\n",
       " 'Structures, and Program Development; Advanced Programming Principles; Machine Architecture and',\n",
       " 'Organization; Algorithms & Data Structures',\n",
       " '']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "section_lines.get(\"education\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'school': 'University of Minnesota - Twin Cities',\n",
       " 'graduationDate': 'May 2020'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_education_section(section_lines.get(\"education\", []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SKILLS',\n",
       " '',\n",
       " '- Java, Python, C/C++, C#, Git, MySQL',\n",
       " '- HTML, CSS, JavaScript, PHP, Sass, LESS',\n",
       " '',\n",
       " '- React, Angular, NodeJS',\n",
       " '- AWS, Drupal, WordPress']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "section_lines.get(\"skills\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'skills': ['Java',\n",
       "  'Python',\n",
       "  'C/C++',\n",
       "  'C#',\n",
       "  'Git',\n",
       "  'MySQL',\n",
       "  'HTML',\n",
       "  'CSS',\n",
       "  'JavaScript',\n",
       "  'PHP',\n",
       "  'Sass',\n",
       "  'LESS',\n",
       "  'React',\n",
       "  'Angular',\n",
       "  'NodeJS',\n",
       "  'AWS',\n",
       "  'Drupal',\n",
       "  'WordPress']}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_skills_section(section_lines.get(\"skills\", []))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_just curious_: what are the most common section headers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('education', 86),\n",
       " ('projects', 45),\n",
       " ('skills', 44),\n",
       " ('experience', 41),\n",
       " ('work experience', 26),\n",
       " ('technical skills', 21),\n",
       " ('awards', 17),\n",
       " ('languages', 10),\n",
       " ('leadership', 9),\n",
       " ('professional experience', 8),\n",
       " ('relevant experience', 4),\n",
       " ('achievements', 4),\n",
       " ('education:', 4),\n",
       " ('activities', 4),\n",
       " ('experience:', 3),\n",
       " ('leadership experience', 3),\n",
       " ('additional experience', 2),\n",
       " ('side projects', 2),\n",
       " ('volunteering', 2),\n",
       " ('experiences', 2),\n",
       " ('honors & awards', 2),\n",
       " ('languages and technologies', 2),\n",
       " ('programming projects', 2),\n",
       " ('projects:', 2),\n",
       " ('tools', 2),\n",
       " ('1', 2)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header_counts = collections.Counter()\n",
    "for fname, text in msvdd_bloc.data.fileio.load_text_files_from_zip(resumes_fpath):\n",
    "    text = preprocess_resume_text(text)\n",
    "    lines = [line.strip() for line in text.split(\"\\n\")]\n",
    "    section_idxs = get_section_idxs(lines)\n",
    "    header_counts.update(lines[idx].lower() for _, idx in section_idxs if idx != len(lines))\n",
    "[item for item in header_counts.most_common() if item[1] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
