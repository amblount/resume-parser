{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Brief: Job posting ingestion pipeline\n",
    "\n",
    "\n",
    "### CoNVO\n",
    "\n",
    "**Context:** Bloc is a career services management platform that builds smart career and data management tools for job-seekers and the organizations serving them. In particular, Bloc seeks to provide and facilitate access to tools for effectively presenting job-seekers' credentials and matching employers' job postings, and thereby improve outcomes.\n",
    "\n",
    "**Need:** Job-seekers need help finding job postings that are aligned with their skills, goals, salary, and location (among other constraints), since that helps to reduce time spent filtering out bad matches and improve the likelihood of a successful outcome. Typical \"fuzzy search\" by job title, maybe with structured filters layered on top, performs decently well, but does take time. Unfortunately, an outside consumer of job postings like Bloc generally has more restricted options in terms of a jobs search.\n",
    "\n",
    "**Vision:** Automated extraction of key information from job postings, and perhaps classification of postings into relevant categories, pulled from external data sources, in order to better match job postings with job-seekers.\n",
    "\n",
    "**Outcome:** A standalone, proof-of-concept process for extracting key job posting information (and categorizing it into relevant classes), and returning it as structured data, complete with unit tests and documentation on expected usage, limitations, and potential improvements.\n",
    "\n",
    "\n",
    "### Data Summary\n",
    "\n",
    "A collection of ~575 job postings for tech+data roles in the NYC area have been fetched from the following external APIs:\n",
    "\n",
    "- [GitHub Jobs API](https://jobs.github.com/api)\n",
    "- [Indeed.com API](https://opensource.indeedeng.io/api-documentation/)\n",
    "- [TheMuse.com API](https://www.themuse.com/developers/api/v2)\n",
    "\n",
    "The APIs all return JSON-formatted data, but with a variety of fields and level of detail. All jobs include structured data for the job title, location, company, and URL, as well as a field containing either all or a short snippet of the job description as text data. These descriptions generally contain information on required / desired applicant qualifications, expected responsibilities and tasks, company missions and benefits, and more -- but snippets don't consistently contain the most important details.\n",
    "\n",
    "\n",
    "### Proposed Methodology\n",
    "\n",
    "The first and simplest task is to transform the structured data from each of the data sources into a consistent representation; in practice, this will entail renaming fields and parsing values (e.g. dates) into a standardized form. The second task is to extract as much relevant information from the free text job descriptions as possible -- key skills, technological expertise, education requirements, etc. -- in a form that allows for matching against job-seekers' skills, expertise, education. A combination of regular expressions, fuzzy string matching, and dictionary/gazetteer lookups, and other basic text processing will hopefully be enough to reliably extract info. The final, possibly optional task is to train text categorizers to predict classes of interest based on the descriptions' text: company culture? flexibility of work-life balance? Given the small size of the dataset, a simple bag-of-words-based approach may be more likely to succeed than, say, a deep learning model.\n",
    "\n",
    "\n",
    "### Definitions of Success\n",
    "\n",
    "- **Baseline:** A function that accepts a job posting from any of the three listed data sources and returns standardized, structured data for key job metadata (title, location, company, and posting URL), plus any available information on skills/expertise/education required or desired for the role. The quality of the extracted values may be messy or not fully structured, but shouldn't contain values from other fields. This function should have basic unit tests and documentation.\n",
    "- **Target:** A function that accepts a job posting from any of the three listed data sources and returns standardized, structured data for key job metadata and available information on required/desired job qualifications (see Baseline), plus any available information on expected responsibilities and company mission/benefits. The quality of the extracted values should be mostly structured -- not large blobs of text, just the most informative bits -- and shouldn't contain values from other fields. This function should have unit tests covering a variety of expected scenarios and good documentation.\n",
    "- **Stretch:** A function as described in Target, plus trained text categorization models for predicting one or more classes of interest. Text preprocessing and model structure must be documented, as well as validation results for model performance.\n",
    "\n",
    "\n",
    "### Risks\n",
    "\n",
    "It's possible that the information included in / extracted from job postings is too complex or varied for sufficiently accurate rules-based parsing, in which case a more sophisticated (ML- or DL-based) approach would be necessary, albeit impractical owing to time and data constraints. It's also possible that a rules-based approach is feasible, but too difficult / large a task for a single day's work.\n",
    "\n",
    "Similarly, job categories that Bloc would find most useful may not be feasible given the size and limitations of the available training dataset. The challenge here is finding categorizations that are still useful but not too abstract/complex for a machine to learn with sufficient accuracy in a single day's work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requests 2.22.0\n",
      "json     2.0.9\n",
      "CPython 3.7.4\n",
      "IPython 7.8.0\n"
     ]
    }
   ],
   "source": [
    "%watermark -v -iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/Users/burtondewilde/Desktop/datakind/bloc/msvdd_Bloc/data/postings\"\n",
    "role_queries = [\n",
    "    \"web developer\",\n",
    "    \"front-end developer\",\n",
    "    \"back-end developer\",\n",
    "    \"full-stack developer\",\n",
    "    \"python\",\n",
    "    \"data scientist\",\n",
    "    \"data engineer\",\n",
    "]\n",
    "category_queries = [\"Engineering\", \"Data Science\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedupe_results(results, dupe_key):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        results ((List[dict]))\n",
    "        dupe_key (str)\n",
    "    \n",
    "    Returns:\n",
    "        List[dict]\n",
    "    \"\"\"\n",
    "    unique_results = []\n",
    "    seen_keys = set()\n",
    "    for result in results:\n",
    "        if result[dupe_key] in seen_keys:\n",
    "            continue\n",
    "        else:\n",
    "            seen_keys.add(result[dupe_key])\n",
    "            unique_results.append(result)\n",
    "    return unique_results\n",
    "\n",
    "\n",
    "def chunk_items(items, chunk_size):\n",
    "    \"\"\"\n",
    "    Yield successive chunks of items.\n",
    "    \n",
    "    Args:\n",
    "        items (list or tuple)\n",
    "        chunk_size (int)\n",
    "    \n",
    "    Yields:\n",
    "        list or tuple\n",
    "    \"\"\"\n",
    "    for i in range(0, len(items), chunk_size):\n",
    "        yield items[i : min(i + chunk_size, len(items))]\n",
    "        \n",
    "    \n",
    "def save_jobs_to_disk(data, filepath):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data (List[dict])\n",
    "        filepath (str)\n",
    "    \"\"\"\n",
    "    with io.open(filepath, mode=\"wt\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GitHub Jobs API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_github_jobs(*, query, location=None, limit=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        query (str): A search term, such as \"ruby\" or \"java\".\n",
    "        location (str): Like \"New York, NY\".\n",
    "        limit (int)\n",
    "        \n",
    "    Returns:\n",
    "        List[dict]\n",
    "    \"\"\"\n",
    "    base_url = \"https://jobs.github.com/positions.json\"\n",
    "    params = {\"description\": query}\n",
    "    if location:\n",
    "        params[\"location\"] = location\n",
    "    results = []\n",
    "    i = 0\n",
    "    while True:\n",
    "        page_params = params.copy()\n",
    "        page_params[\"page\"] = i\n",
    "        try:\n",
    "            response = requests.get(base_url, params=page_params)\n",
    "            response.raise_for_status()\n",
    "        except requests.RequestException as e:\n",
    "            print(e)\n",
    "            break\n",
    "        page_results = response.json()\n",
    "        results.extend(page_results)\n",
    "        i += 1\n",
    "        if len(page_results) < 50:\n",
    "            break\n",
    "        elif limit is not None and len(results) > limit:\n",
    "            break\n",
    "    if limit is not None:\n",
    "        return results[:limit]\n",
    "    else:\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: web developer\n",
      "query: front-end developer\n",
      "query: back-end developer\n",
      "query: full-stack developer\n",
      "query: python\n",
      "query: data scientist\n",
      "query: data engineer\n",
      "# jobs fetched = 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': '3e378b91-7cc0-486e-b18d-518ab29569e3',\n",
       "  'type': 'Full Time',\n",
       "  'url': 'https://jobs.github.com/positions/3e378b91-7cc0-486e-b18d-518ab29569e3',\n",
       "  'created_at': 'Wed Sep 11 18:59:59 UTC 2019',\n",
       "  'company': 'Simon & Schuster ',\n",
       "  'company_url': 'https://www.simonandschuster.com/',\n",
       "  'location': 'New York, NY',\n",
       "  'title': 'Lead Software Engineer',\n",
       "  'description': '<p>Simon &amp; Schuster is seeking a Lead Software Engineer to join a rapidly growing team focused on impacting the world of publishing through research and innovation. Working with a team of data scientists, data engineers, designers, and domain experts, you will be involved in rapidly prototyping, developing, and deploying the platforms that put insights and information into the hands of decision-makers.</p>\\n<p>From databases to serverless applications, you will be designing, deploying , and managing the systems that connect our real-world data to the books and authors that inform and entertain our world. As a Full Stack (Engineer|Developer) you will engage with engineers and domain experts across the organization, collaborating to design and deploy innovative new products and services. From inception to production, you will onboard, integrate and deploy an array of cutting-edge tools and technologies to seamlessly deliver our solutions.</p>\\n<p>Publishing experience is not required. Instead we are seeking teammates with curiosity, problem-solving skills, and a thirst for knowledge.</p>\\n<p>Responsibilities:</p>\\n<p>●     Scale team success from prototype to production.\\n●     Own the software engineering aspects of our data services and applications.\\n●     Design, build, test, and deploy solutions serving the entire organization.\\n●     Be the “go to” engineer for our products and services.\\n●     Tailor and drive the creation and adoption of best practices for software engineering for our team and peer engineering teams across the organization.\\n●     Work with users, designers, developers, and analysts to ensure requirements-driven engineering in all we do.\\n●     Ensure high availability and security are maintained in the products and services delivered by our team.</p>\\n<p>Qualifications:\\n●     5+ years experience in web application development and managing deployments\\n●      Experience with full-stack development: Front-end, web servers, databases and storage.\\n●      Deep understanding of modern software development practices (CI/CD, Continuous Monitoring, Testing, Documentation)\\n●     Experience in production commercial with architecting and managing cloud deployments (GCP, AWS, Azure, etc.)\\n●     Proficiency with Linux, SQL, Python, and Javascript required\\n●     Solid background working with large-scale and fast-moving data technologies (Streaming, NoSQL, Object-stores, etc.)\\n●     Familiarity with common build systems, Github/Gitlab, Confluence, JIRA\\n●     Ability to work effectively in teams of technical and non-technical people.\\n●     Ability to work independently and effectively\\n●     Desire to research and communicate requirements and solutions.</p>\\n',\n",
       "  'how_to_apply': '<p>Apply Online: <a href=\"https://cbs.avature.net/cbssscareers/JobDetail/Lead-Software-Engineer/14487\">https://cbs.avature.net/cbssscareers/JobDetail/Lead-Software-Engineer/14487</a></p>\\n',\n",
       "  'company_logo': 'https://jobs.github.com/rails/active_storage/blobs/eyJfcmFpbHMiOnsibWVzc2FnZSI6IkJBaHBBdDF4IiwiZXhwIjpudWxsLCJwdXIiOiJibG9iX2lkIn19--acf24dd4bdd4f94a7d7a2dfc717ef5415bad86ce/43811-1.jpg'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_search_results = []\n",
    "for query in role_queries:\n",
    "    print(\"query:\", query)\n",
    "    try:\n",
    "        results = search_github_jobs(query=query, location=\"New York, NY\", limit=100)\n",
    "        all_search_results.extend(results)\n",
    "    except Exception:\n",
    "        break\n",
    "\n",
    "# deduplicate the list of search results\n",
    "github_jobs = dedupe_results(all_search_results, \"id\")\n",
    "print(\"# jobs fetched =\", len(github_jobs))\n",
    "github_jobs[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_jobs_to_disk(github_jobs, os.path.join(data_dir, \"github_jobs.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indeed.com API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_indeed_jobs(publisher_id, user_ip, *, query, location=None, limit=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        publisher_id (str)\n",
    "        user_ip (str)\n",
    "        query (str): A search term, such as \"ruby\" or \"java\".\n",
    "        location (str): Like \"New York, NY\".\n",
    "        limit (int)\n",
    "    \n",
    "    Returns:\n",
    "        List[dict]\n",
    "    \"\"\"\n",
    "    base_url = \"http://api.indeed.com/ads/apisearch\"\n",
    "    results_per_page = 25\n",
    "    params = {\n",
    "        \"publisher\": publisher_id,\n",
    "        \"userip\": user_ip,\n",
    "        \"useragent\": \"blocbot python/v1\",\n",
    "        \"v\": 2,\n",
    "        \"format\": \"json\",\n",
    "        \"filter\": 1,\n",
    "        \"limit\": results_per_page,  # not the same as `limit` kwarg\n",
    "    }\n",
    "    params[\"q\"] = query\n",
    "    if location:\n",
    "        params[\"l\"] = location\n",
    "    start = 0\n",
    "    results = []\n",
    "    while True:\n",
    "        page_params = params.copy()\n",
    "        page_params[\"start\"] = start\n",
    "        try:\n",
    "            response = requests.get(base_url, params=page_params)\n",
    "            response.raise_for_status()\n",
    "        except requests.RequestException as e:\n",
    "            print(e)\n",
    "            break\n",
    "        page_results = response.json().get(\"results\", [])\n",
    "        results.extend(page_results)\n",
    "        start += results_per_page\n",
    "        if len(page_results) < results_per_page:\n",
    "            break\n",
    "        elif limit is not None and len(results) > limit:\n",
    "            break\n",
    "    if limit is not None:\n",
    "        results = results[:limit]\n",
    "    \n",
    "    detailed_results = []\n",
    "    job_keys = [result[\"jobkey\"] for result in results]\n",
    "    base_url = \"http://api.indeed.com/ads/apigetjobs\"\n",
    "    params = {\n",
    "        \"publisher\": publisher_id,\n",
    "        \"userip\": user_ip,\n",
    "        \"v\": 2,\n",
    "        \"format\": \"json\",\n",
    "    }\n",
    "    for job_keys_chunk in chunk_items(job_keys, 10):\n",
    "        chunk_params = params.copy()\n",
    "        chunk_params[\"jobkeys\"] = \",\".join(job_key for job_key in job_keys_chunk)\n",
    "        try:\n",
    "            response = requests.get(base_url, params=chunk_params)\n",
    "            response.raise_for_status()\n",
    "        except requests.RequestException as e:\n",
    "            print(e)\n",
    "            break\n",
    "        chunk_results = response.json().get(\"results\", [])\n",
    "        detailed_results.extend(chunk_results)\n",
    "    return detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with io.open(\"/Users/burtondewilde/.bloc/indeed\", mode=\"rt\") as f:\n",
    "    publisher_id = f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ip = requests.get(\"https://api.ipify.org\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: web developer\n",
      "query: front-end developer\n",
      "query: back-end developer\n",
      "query: full-stack developer\n",
      "query: python\n",
      "query: data scientist\n",
      "query: data engineer\n",
      "# jobs fetched = 443\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'jobtitle': 'Front-End Developer',\n",
       "  'company': 'The New Republic',\n",
       "  'city': 'New York',\n",
       "  'state': 'NY',\n",
       "  'country': 'US',\n",
       "  'language': 'en',\n",
       "  'formattedLocation': 'New York, NY',\n",
       "  'source': 'The New Republic',\n",
       "  'date': 'Sat, 14 Sep 2019 15:30:05 GMT',\n",
       "  'snippet': 'The front-end developer will identify, design, and build editorial initiatives—from news projects and social campaigns to daily newsletters—as well as events, sales, and marketing products. Collaborate closely with key staff to design and implement products for mobile, tablet, and desktop, from an extensive redesign to sales and marketing integrations....',\n",
       "  'url': 'http://www.indeed.com/rc/clk?jk=8986a15fbb5aa6b8&atk=',\n",
       "  'onmousedown': \"indeed_clk(this,'');\",\n",
       "  'latitude': 40.73551,\n",
       "  'longitude': -73.9919,\n",
       "  'jobkey': '8986a15fbb5aa6b8',\n",
       "  'sponsored': False,\n",
       "  'expired': False,\n",
       "  'indeedApply': True,\n",
       "  'formattedLocationFull': 'New York, NY 10003',\n",
       "  'formattedRelativeTime': '8 days ago',\n",
       "  'stations': '',\n",
       "  'recommendations': []}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_search_results = []\n",
    "for query in role_queries:\n",
    "    print(\"query:\", query)\n",
    "    try:\n",
    "        results = search_indeed_jobs(publisher_id, user_ip, query=query, location=\"New York, NY\", limit=100)\n",
    "        all_search_results.extend(results)\n",
    "    except Exception:\n",
    "        break\n",
    "\n",
    "# deduplicate the list of search results\n",
    "indeed_jobs = dedupe_results(all_search_results, \"jobkey\")\n",
    "print(\"# jobs fetched =\", len(indeed_jobs))\n",
    "indeed_jobs[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_jobs_to_disk(indeed_jobs, os.path.join(data_dir, \"indeed_jobs.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Muse API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_themuse_jobs(*, category, location=None, limit=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        category (str): Job category to search.\n",
    "        location (str): Like \"New York, NY\".\n",
    "        limit (int)\n",
    "        \n",
    "    Returns:\n",
    "        List[dict]\n",
    "    \"\"\"\n",
    "    if category not in _CATEGORIES:\n",
    "        raise ValueError(\n",
    "            \"category={} is invalid; valid values are {}\".format(category, _CATEGORIES))\n",
    "    base_url = \"https://www.themuse.com/api/public/jobs\"\n",
    "    params = {\"category\": category}\n",
    "    if location:\n",
    "        params[\"location\"] = location\n",
    "    results = []\n",
    "    i = 0\n",
    "    while True:\n",
    "        page_params = params.copy()\n",
    "        page_params[\"page\"] = i\n",
    "        try:\n",
    "            response = requests.get(base_url, params=page_params)\n",
    "            response.raise_for_status()\n",
    "        except requests.RequestException as e:\n",
    "            print(e)\n",
    "            break\n",
    "        data = response.json()\n",
    "        page_results = data.get(\"results\", [])\n",
    "        results.extend(page_results)\n",
    "        i += 1\n",
    "        if len(page_results) < data[\"items_per_page\"]:\n",
    "            break\n",
    "        elif limit is not None and len(results) > limit:\n",
    "            break\n",
    "    if limit is not None:\n",
    "        return results[:limit]\n",
    "    else:\n",
    "        return results\n",
    "    \n",
    "    \n",
    "_CATEGORIES = {\n",
    "    \"Account Management\",\n",
    "    \"Business & Strategy\",\n",
    "    \"Creative & Design\",\n",
    "    \"Customer Service\",\n",
    "    \"Data Science\",\n",
    "    \"Editorial\",\n",
    "    \"Education\",\n",
    "    \"Engineering\",\n",
    "    \"Finance\",\n",
    "    \"Fundraising & Development\",\n",
    "    \"Healthcare & Medicine\",\n",
    "    \"HR & Recruiting\",\n",
    "    \"Legal\",\n",
    "    \"Marketing & PR\",\n",
    "    \"Operations\",\n",
    "    \"Project & Product Management\",\n",
    "    \"Retail\",\n",
    "    \"Sales\",\n",
    "    \"Social Media & Community\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category: Engineering\n",
      "category: Data Science\n",
      "# jobs fetched = 116\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'contents': \"<p><br><br>The Marquee team at Goldman Sachs delivers digital products and services to our institutional clients. We design and build highly-scalable solutions allowing access to Goldman Sachs' content, data, analytics, risk and execution services. These solutions transform client experiences while generating new revenue streams and business models. Our strategic initiatives disrupt the status-quo by innovating new businesses that unlock the power of Goldman Sachs for our clients. <br><br>Our team of engineers, designers, product managers and salespeople collaborate to drive progress within digital finance. We surround ourselves with big thinkers who thrive on tackling fresh challenges and uniting around a common vision. We move fast, embrace change and welcome fresh perspectives. Help us build the future of finance while positioned at the center of global markets. <br><br>Marquee Sales is accountable for all aspects of the digital sales cycle, including lead generation, initial client engagement, client demos, agreeing commercial terms, documentation and closing. You will be integral to all parts of the digital sales cycle. Confidence and enjoyment of presenting digital product to clients, and keenness to close, are critical. <br><br><strong> RESPONSIBILITIES AND QUALIFICATIONS </strong> <br><br><strong>Responsibilities</strong> <br><ul><li> Sell analytics and risk products and services across asset classes. You will work closely with other sales people and product experts who will help you learn the specifics </li><li> Sit within a global team, supporting senior salespeople in covering and owning client relationships. Marquee Sales is responsible for all aspects of a client's relationship with Marquee, from initial engagement around Marquee's capabilities through to closing the first deployment of a Marquee product, cross-selling other Marquee products and services, and managing the long-term client engagement </li><li> Marquee's clients have various touchpoints within Goldman Sachs across asset classes and business divisions. You will build and leverage your internal network to ensure proper coordination and delivery of best-in-class client experiences </li><li> Track and manage your sales pipeline and be accountable together with the global team for sales metrics and relevant OKRs </li><li> Advocate for your clients' needs </li><li> Help mobilize collaboration across functions, product teams and levels of seniority within the organization </li></ul><br> <strong> </strong><strong>Qualifications</strong> <br><ul><li> Client-centric and has proven relationship-building / networking skills </li><li> Dynamic, confident and tenacious, able to carry a sale through the complete sales cycle and own the relevant metrics </li><li> Displays strong organization and communication (oral and written) skills </li><li> Collaborate efficiently within a diverse global team spanning multiple continents </li><li> Foundational knowledge of at least one major asset class (Equities, Rates, FX, Commodities, Credit) </li><li> Experience of a tech-focused business or business initiative </li></ul> <br><br><strong> ABOUT GOLDMAN SACHS </strong> <br><br>The Goldman Sachs Group, Inc. is a leading global investment banking, securities and investment management firm that provides a wide range of financial services to a substantial and diversified client base that includes corporations, financial institutions, governments and individuals. Founded in 1869, the firm is headquartered in New York and maintains offices in all major financial centers around the world. <br><br> © The Goldman Sachs Group, Inc., 2019. All rights reserved Goldman Sachs is an equal employment/affirmative action employer Female/Minority/Disability/Vet. <br><br></p>\",\n",
       "  'name': 'Marquee - Sales',\n",
       "  'type': 'external',\n",
       "  'publication_date': '2019-08-29T05:45:37.425687Z',\n",
       "  'short_name': 'marquee-sales',\n",
       "  'model_type': 'jobs',\n",
       "  'id': 2147387,\n",
       "  'locations': [{'name': 'New York, NY'}],\n",
       "  'categories': [{'name': 'Engineering'}],\n",
       "  'levels': [],\n",
       "  'tags': [{'name': 'Fortune 1000', 'short_name': 'fortune-1000-companies'}],\n",
       "  'refs': {'landing_page': 'https://www.themuse.com/jobs/goldmansachs/marquee-sales'},\n",
       "  'company': {'id': 850,\n",
       "   'short_name': 'goldmansachs',\n",
       "   'name': 'Goldman Sachs'}}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_search_results = []\n",
    "for category in category_queries:\n",
    "    print(\"category:\", category)\n",
    "    try:\n",
    "        results = search_themuse_jobs(category=category, location=\"New York, NY\", limit=100)\n",
    "        all_search_results.extend(results)\n",
    "    except Exception:\n",
    "        break\n",
    "\n",
    "# deduplicate the list of search results\n",
    "themuse_jobs = dedupe_results(all_search_results, \"id\")\n",
    "print(\"# jobs fetched =\", len(themuse_jobs))\n",
    "themuse_jobs[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_jobs_to_disk(themuse_jobs, os.path.join(data_dir, \"themuse_jobs.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### combined dataset\n",
    "\n",
    "(assuming no duplicates across sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total # of jobs fetched =  579\n"
     ]
    }
   ],
   "source": [
    "print(\"total # of jobs fetched = \", len(github_jobs + indeed_jobs + themuse_jobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
