{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Basic Usage\n",
    "\n",
    "Transform a résumé file (PDF) into structured data (JSON)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import msvdd_bloc.resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = msvdd_bloc.ROOT_DIR.joinpath(\"tests\", \"data\", \"fake-resume.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake-resume\n",
      "\n",
      "\n",
      "JOHN DOE \n",
      "123-456-7890    |    john.doe@fake.com    |    123 Fake St. Apt 456, Fake City, BS 78910 \n",
      "\n",
      "SUMMARY \n",
      "Hard-working, self-motivated individual seeking a position in the field of data science to \n",
      "develop my professional skills and do good in the world. \n",
      "\n",
      "EXPERIENCE \n",
      "\n",
      "Senior Data Scientist,  Bloc;  Chicago, IL    |    Aug 2019 – Present \n",
      "\n",
      "• Led a team of volunteers to hack on a data project that proved trickier than anticipated \n",
      "\n",
      "• Developed code to generate fake data and train real models to parse résumé, then \n",
      "wrote documentation, tests, and scripts for successful usage of project outputs \n",
      "\n",
      "Data Scientist,  Datakind;  New York, NY    |    Mar. 2015 – Aug. 2019 \n",
      "\n",
      "Data do-gooder who just can’t say no to Jake Porway. Contributed to many projects in \n",
      "many roles, from Data Creative scoping out potential work to event photographer at one \n",
      "of DataKind’s biggest gatherings ever. \n",
      "\n",
      "EDUCATION \n",
      "University of Fake State – Fake City    |    Sep 2007 - Aug 2012 \n",
      "\n",
      "Ph.D. in Physics; Minor in Applied Math \n",
      "\n",
      "Fake City College    |    2003–2007 \n",
      "\n",
      "• B.S. in Computer Engineering \n",
      "\n",
      "• GPA: 3.9 / 4.0 \n",
      "\n",
      "• Related Courses: Data Structures & Algorithms, Database Administration, Coding 101, \n",
      "Exploratory Data Analysis \n",
      "\n",
      "SKILLS \n",
      "\n",
      "• Programming Languages: Python, SQL, HTML/CSS \n",
      "\n",
      "• English (native), Spanish (conversational), French (basic)\n"
     ]
    }
   ],
   "source": [
    "resume_text = msvdd_bloc.resumes.extract_text_from_pdf(str(filepath))\n",
    "print(resume_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'basics': {'email': 'john.doe@fake.com',\n",
      "            'location': {'address': '123 Fake St. Apt 456',\n",
      "                         'city': 'Fake City',\n",
      "                         'postal_code': '78910',\n",
      "                         'region': 'BS'},\n",
      "            'name': 'JOHN DOE',\n",
      "            'phone': '123-456-7890',\n",
      "            'summary': 'Hard-working, self-motivated individual seeking a position in the field of data science to\\n'\n",
      "                       'develop my professional skills and do good in the world.'},\n",
      " 'education': [{'area': 'Physics; Minor in Applied Math',\n",
      "                'end_date': 'Aug 2012',\n",
      "                'institution': 'University of Fake State – Fake City',\n",
      "                'start_date': 'Sep 2007',\n",
      "                'study_type': 'Ph.D.'},\n",
      "               {'area': 'Computer Engineering',\n",
      "                'courses': ['Data Structures & Algorithms',\n",
      "                            'Database Administration',\n",
      "                            'Coding 101',\n",
      "                            'Exploratory Data Analysis'],\n",
      "                'end_date': '2003',\n",
      "                'gpa': '3.9 / 4.0',\n",
      "                'institution': 'Fake City College',\n",
      "                'start_date': '2007',\n",
      "                'study_type': 'B.S.'}],\n",
      " 'skills': [{'keywords': ['Python', 'SQL', 'HTML/CSS'], 'name': 'Programming Languages'},\n",
      "            {'level': 'native', 'name': 'English'},\n",
      "            {'level': 'conversational', 'name': 'Spanish'},\n",
      "            {'level': 'basic', 'name': 'French'}],\n",
      " 'work': [{'company': 'Bloc',\n",
      "           'end_date': 'Present',\n",
      "           'highlights': ['Led a team of volunteers to hack on a data project that proved trickier than anticipated',\n",
      "                          'Developed code to generate fake data and train real models to parse résumé, then',\n",
      "                          'wrote documentation, tests, and scripts for successful usage of project outputs'],\n",
      "           'position': 'Senior Data Scientist',\n",
      "           'start_date': 'Aug 2019'},\n",
      "          {'company': 'Datakind',\n",
      "           'end_date': 'Aug. 2019',\n",
      "           'highlights': [\"Data do-gooder who just can't say no to Jake Porway. Contributed to many projects in\",\n",
      "                          'many roles, from Data Creative scoping out potential work to event photographer at one',\n",
      "                          \"of DataKind's biggest gatherings ever.\"],\n",
      "           'position': 'Data Scientist',\n",
      "           'start_date': 'Mar. 2015'}]}\n"
     ]
    }
   ],
   "source": [
    "resume_data = msvdd_bloc.resumes.parse_text(resume_text)\n",
    "pprint(resume_data, width=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# How it Works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, the extracted résumé text is first cleaned up and standardized, then split into lines that are associated with a particular section, such as \"basics\" or \"education\". Each section's lines is then tokenized into constituent words, featurized into sequences of numeric and categorical features, then individually tagged with labels such as \"name\" or \"institution\". These sequences of labeled tokens are then parsed into structured data, which often involves filtering out \"junk\" tokens and combining like adjacent tokens into contiguous text strings, such as \"JOHN DOE\" and \"University of Fake State – Fake City\". Lastly, the resulting data is validated according to a declared schema.\n",
    "\n",
    "The code for the high-level parsing function is relatively straightforward, on its surface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mmsvdd_bloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresumes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0mparse_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
       "\u001b[0;34m    Parse raw extracted résumé ``text`` into structured data conforming to the schema\u001b[0m\n",
       "\u001b[0;34m    specified in :class:`schemas.ResumeSchema()`.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Args:\u001b[0m\n",
       "\u001b[0;34m        text (str)\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Returns:\u001b[0m\n",
       "\u001b[0;34m        Dict[str, object]\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnorm_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmunge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtext_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmunge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_filtered_text_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msection_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_section_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# if we don't get any sections besides the default, something's gone wrong\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msection_lines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"start\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unable to parse résumé text\\n%s ...\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# basics section\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbasics_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msection_lines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"start\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msection_lines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"basics\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbasics_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbasics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasics_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# NOTE: uncomment if summary section is split out from main basics lines\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# if section_lines.get(\"summary\"):\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m#     basics_data[\"summary\"] = \"\\n\".join(section_lines[\"summary\"]).strip()\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"basics\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbasics_data\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# education\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0meducation_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msection_lines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"education\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0meducation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meducation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meducation_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"education\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meducation_data\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# NOTE: uncomment if courses subsection is split out from main education lines\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# courses_lines = section_lines.get(\"courses\", [])\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# skills\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mskills_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msection_lines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"skills\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mskills_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskills\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskills_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"skills\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskills_data\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# work\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mwork_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msection_lines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"work\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mwork_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwork_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"work\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwork_data\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# TODO: figure out what we want to do here\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# option 1: validate and warn, but return data as-is\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# validation = _RESUME_SCHEMA.validate(data)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# if validation:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m#     LOGGER.warning(\"validation error: %s\", validation)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# option 2: validate and warn, but only return valid data\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_RESUME_SCHEMA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mexcept\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mValidationError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"validation error: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Desktop/datakind/bloc/msvdd_Bloc/msvdd_bloc/resumes/parse.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "msvdd_bloc.resumes.parse_text??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing text clears up any text encoding weirdness, truncates long stretches of spaces/newlines, and very importantly, transforms a wide variety of bullet symbols into a consistent `-`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s & Algorithms, Database Administration, Coding 101, \n",
      "Exploratory Data Analysis \n",
      "\n",
      "SKILLS \n",
      "\n",
      "- Programming Languages: Python, SQL, HTML/CSS \n",
      "\n",
      "- English (native), Spanish (conversational), French (basic)\n"
     ]
    }
   ],
   "source": [
    "norm_text = msvdd_bloc.resumes.munge.normalize_text(resume_text)\n",
    "text_lines = msvdd_bloc.resumes.munge.get_filtered_text_lines(norm_text)\n",
    "print(norm_text[-200:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we iterate through lines of normalized text, searching for matches to known patterns that resemble section headers, like `\"Education:\"` or `\"SKILLS\"`. These patterns are expressed as regular expressions, and are easily updated if a new, unambiguous pattern is encountered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '- Programming Languages: Python, SQL, HTML/CSS',\n",
       " '- English (native), Spanish (conversational), French (basic)']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "section_lines = msvdd_bloc.resumes.segment.get_section_lines(text_lines)\n",
    "section_lines[\"skills\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A given section's lines are then tokenized — that is, split into individual \"words\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-,\n",
       " Programming,\n",
       " Languages,\n",
       " :,\n",
       " Python,\n",
       " ,,\n",
       " SQL,\n",
       " ,,\n",
       " HTML,\n",
       " /,\n",
       " CSS,\n",
       " ,\n",
       " -,\n",
       " English,\n",
       " (,\n",
       " native,\n",
       " ),\n",
       " ,,\n",
       " Spanish,\n",
       " (,\n",
       " conversational,\n",
       " ),\n",
       " ,,\n",
       " French,\n",
       " (,\n",
       " basic,\n",
       " )]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = msvdd_bloc.tokenize.tokenize(\"\\n\".join(section_lines[\"skills\"]).strip())\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each token is then transformed into a collection of numerical and categorical features that a model can use to make good predictions about its label. These features include its position in the sequence, text length, its case, whether or not its entirely punctuation or whitespace, like a number or email, and more. A token's immediate neighbors' features are also added to its own, nested under a \"prev\" or \"next\" key to keep everything separate. And, depending on the section, additional section-specific features may be added, such as whether or not a token looks like a month or year, or is a word commonly used to indicate one's level of proficiency on a skill.\n",
    "\n",
    "As an example, here's what the `Python` token's feature set looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': 4,\n",
       " 'len': 6,\n",
       " 'shape': 'Xxxxx',\n",
       " 'prefix': 'P',\n",
       " 'suffix': 'hon',\n",
       " 'is_alpha': True,\n",
       " 'is_digit': False,\n",
       " 'is_lower': False,\n",
       " 'is_upper': False,\n",
       " 'is_title': True,\n",
       " 'is_punct': False,\n",
       " 'is_left_punct': False,\n",
       " 'is_right_punct': False,\n",
       " 'is_bracket': False,\n",
       " 'is_quote': False,\n",
       " 'is_space': False,\n",
       " 'like_num': False,\n",
       " 'like_url': False,\n",
       " 'like_email': False,\n",
       " 'is_stop': False,\n",
       " 'is_alnum': True,\n",
       " 'is_newline': False,\n",
       " 'is_partial_digit': False,\n",
       " 'is_partial_punct': False,\n",
       " 'is_group_sep_text': False,\n",
       " 'is_item_sep_text': False,\n",
       " 'is_level_text': False,\n",
       " 'ppprev': {'idx': 1,\n",
       "  'len': 11,\n",
       "  'shape': 'Xxxxx',\n",
       "  'prefix': 'P',\n",
       "  'suffix': 'ing',\n",
       "  'is_alpha': True,\n",
       "  'is_digit': False,\n",
       "  'is_lower': False,\n",
       "  'is_upper': False,\n",
       "  'is_title': True,\n",
       "  'is_punct': False,\n",
       "  'is_left_punct': False,\n",
       "  'is_right_punct': False,\n",
       "  'is_bracket': False,\n",
       "  'is_quote': False,\n",
       "  'is_space': False,\n",
       "  'like_num': False,\n",
       "  'like_url': False,\n",
       "  'like_email': False,\n",
       "  'is_stop': False,\n",
       "  'is_alnum': True,\n",
       "  'is_newline': False,\n",
       "  'is_partial_digit': False,\n",
       "  'is_partial_punct': False,\n",
       "  'is_group_sep_text': False,\n",
       "  'is_item_sep_text': False,\n",
       "  'is_level_text': False},\n",
       " 'pprev': {'idx': 2,\n",
       "  'len': 9,\n",
       "  'shape': 'Xxxxx',\n",
       "  'prefix': 'L',\n",
       "  'suffix': 'ges',\n",
       "  'is_alpha': True,\n",
       "  'is_digit': False,\n",
       "  'is_lower': False,\n",
       "  'is_upper': False,\n",
       "  'is_title': True,\n",
       "  'is_punct': False,\n",
       "  'is_left_punct': False,\n",
       "  'is_right_punct': False,\n",
       "  'is_bracket': False,\n",
       "  'is_quote': False,\n",
       "  'is_space': False,\n",
       "  'like_num': False,\n",
       "  'like_url': False,\n",
       "  'like_email': False,\n",
       "  'is_stop': False,\n",
       "  'is_alnum': True,\n",
       "  'is_newline': False,\n",
       "  'is_partial_digit': False,\n",
       "  'is_partial_punct': False,\n",
       "  'is_group_sep_text': False,\n",
       "  'is_item_sep_text': False,\n",
       "  'is_level_text': False},\n",
       " 'prev': {'idx': 3,\n",
       "  'len': 1,\n",
       "  'shape': ':',\n",
       "  'prefix': ':',\n",
       "  'suffix': ':',\n",
       "  'is_alpha': False,\n",
       "  'is_digit': False,\n",
       "  'is_lower': False,\n",
       "  'is_upper': False,\n",
       "  'is_title': False,\n",
       "  'is_punct': True,\n",
       "  'is_left_punct': False,\n",
       "  'is_right_punct': False,\n",
       "  'is_bracket': False,\n",
       "  'is_quote': False,\n",
       "  'is_space': False,\n",
       "  'like_num': False,\n",
       "  'like_url': False,\n",
       "  'like_email': False,\n",
       "  'is_stop': False,\n",
       "  'is_alnum': False,\n",
       "  'is_newline': False,\n",
       "  'is_partial_digit': False,\n",
       "  'is_partial_punct': False,\n",
       "  'is_group_sep_text': True,\n",
       "  'is_item_sep_text': False,\n",
       "  'is_level_text': False},\n",
       " 'next': {'idx': 5,\n",
       "  'len': 1,\n",
       "  'shape': ',',\n",
       "  'prefix': ',',\n",
       "  'suffix': ',',\n",
       "  'is_alpha': False,\n",
       "  'is_digit': False,\n",
       "  'is_lower': False,\n",
       "  'is_upper': False,\n",
       "  'is_title': False,\n",
       "  'is_punct': True,\n",
       "  'is_left_punct': False,\n",
       "  'is_right_punct': False,\n",
       "  'is_bracket': False,\n",
       "  'is_quote': False,\n",
       "  'is_space': False,\n",
       "  'like_num': False,\n",
       "  'like_url': False,\n",
       "  'like_email': False,\n",
       "  'is_stop': False,\n",
       "  'is_alnum': False,\n",
       "  'is_newline': False,\n",
       "  'is_partial_digit': False,\n",
       "  'is_partial_punct': False,\n",
       "  'is_group_sep_text': False,\n",
       "  'is_item_sep_text': True,\n",
       "  'is_level_text': False},\n",
       " 'nnext': {'idx': 6,\n",
       "  'len': 3,\n",
       "  'shape': 'XXX',\n",
       "  'prefix': 'S',\n",
       "  'suffix': 'SQL',\n",
       "  'is_alpha': True,\n",
       "  'is_digit': False,\n",
       "  'is_lower': False,\n",
       "  'is_upper': True,\n",
       "  'is_title': False,\n",
       "  'is_punct': False,\n",
       "  'is_left_punct': False,\n",
       "  'is_right_punct': False,\n",
       "  'is_bracket': False,\n",
       "  'is_quote': False,\n",
       "  'is_space': False,\n",
       "  'like_num': False,\n",
       "  'like_url': False,\n",
       "  'like_email': False,\n",
       "  'is_stop': False,\n",
       "  'is_alnum': True,\n",
       "  'is_newline': False,\n",
       "  'is_partial_digit': False,\n",
       "  'is_partial_punct': False,\n",
       "  'is_group_sep_text': False,\n",
       "  'is_item_sep_text': False,\n",
       "  'is_level_text': False},\n",
       " 'tok_line_idx': 4,\n",
       " 'follows_bullet': False,\n",
       " 'follows_group_sep': True}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = msvdd_bloc.resumes.skills.parse.featurize(tokens)\n",
    "[features for token, features in zip(tokens, features) if token.text == \"Python\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, a trained model can use the sequence of features to make per-token predictions about the most likely labels, because it's learned the patterns found in a training dataset that map features to labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-, 'field_sep'),\n",
       " (Programming, 'name'),\n",
       " (Languages, 'name'),\n",
       " (:, 'field_sep'),\n",
       " (Python, 'keyword'),\n",
       " (,, 'item_sep'),\n",
       " (SQL, 'keyword'),\n",
       " (,, 'item_sep'),\n",
       " (HTML, 'keyword'),\n",
       " (/, 'keyword')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_tokens = msvdd_bloc.resumes.parse_utils.tag(\n",
    "    tokens, features,\n",
    "    tagger=msvdd_bloc.resumes.parse_utils.load_tagger(msvdd_bloc.resumes.skills.FPATH_TAGGER),\n",
    ")\n",
    "labeled_tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, this sequence of (token, label) pairs has to be parsed using rules that combine the tokens into structured fields. Each section is parsed differently, depending on the structure and relationships of its constituent fields. Here's how that looks for this skills section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Programming Languages', 'keywords': ['Python', 'SQL', 'HTML/CSS']},\n",
       " {'name': 'English', 'level': 'native'},\n",
       " {'name': 'Spanish', 'level': 'conversational'},\n",
       " {'name': 'French', 'level': 'basic'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills_data = msvdd_bloc.resumes.skills.parse._parse_labeled_tokens(labeled_tokens)\n",
    "skills_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":tada:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Generating Fake Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The section-specific models used to label each token in a section's lines are [Conditional Random Field](https://en.wikipedia.org/wiki/Conditional_random_field) models, a type of statistical model that predicts items in a sequence while taking their context — i.e. their _neighbors_ — into account. The relationships between tokens-in-context and their labels are learned from already labeled training data; to learn more complicated relationships, models typically require more training data. Since Bloc's supply of real résumés is relatively limited, we make do by generating sufficiently realistic fakes and assigning known labels.\n",
    "\n",
    "Each résumé section has functionality for randomly generating values for a variety of fields, building upon the framework of the [`faker` package](https://faker.readthedocs.io/en/master/). Field value generators are linked to field keys (shorthand names used as placeholders in template strings) and field labels (the labels we want to predict with a CRF model) by way of a `FIELDS` dictionary. Finally, sequences of fields are generated in randomized template strings, where each field follows the format `{field_key:field_label:probability}`. (Note: the field label and probability components are optional. The default field label is specified in `FIELDS`, and the default probability is 1.0 — as in, it will be generated every time.) A given section comes in many different forms, which entails many different template strings.\n",
    "\n",
    "For example, a template like `\"{uni} {fsep} {city_state} {fsep} {dt}\"` will produce sequences of labeled tokens like these:\n",
    "\n",
    "```\n",
    "[\n",
    "    [\n",
    "        ('State', 'institution'),\n",
    "        ('University', 'institution'),\n",
    "        ('of', 'institution'),\n",
    "        ('West', 'institution'),\n",
    "        ('Virginia', 'institution'),\n",
    "        (',', 'institution'),\n",
    "        ('Harrisonville', 'institution'),\n",
    "        ('   ', 'field_sep'),\n",
    "        (',', 'field_sep'),\n",
    "        ('  ', 'field_sep'),\n",
    "        ('January', 'end_date'),\n",
    "        ('1989', 'end_date')\n",
    "    ],\n",
    "    [\n",
    "        ('College', 'institution'),\n",
    "        ('of', 'institution'),\n",
    "        ('Graceview', 'institution'),\n",
    "        ('  ', 'field_sep'),\n",
    "        (';', 'field_sep'),\n",
    "        (' ', 'field_sep'),\n",
    "        ('Nov.', 'end_date'),\n",
    "        ('2004', 'end_date')\n",
    "    ],\n",
    "]\n",
    "```\n",
    "\n",
    "By using many different templates, many different fakes of a given section can be produced, ideally with enough variation in structure and values to effectively model real data. Here's how that looks in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from msvdd_bloc.resumes import education\n",
    "from msvdd_bloc.resumes import generate_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[fake 0]\n",
      "Doctorate ,   Political Science , Minor , Journalism , Media Studies and Communication \n",
      " Community College of Vermont   –   Rileychester , NC \n",
      " Feb 2005 - Present\n",
      "\n",
      "[fake 1]\n",
      "Guerra University ,    Mendozaton , Oklahoma    Expected Graduation : Mar 2015   – Current \n",
      " AA Culinary Arts \n",
      " Current GPA : 1.23\n",
      "\n",
      "[fake 2]\n",
      "Nguyen State University \n",
      " Recent Courses- Environmental Studies and Policy ,   Ethnic and Gender Studies ,   Contemporary Resource Management \n",
      " Minor ; Natural Sciences \n",
      "\n",
      " Reese Polytechnic University \n",
      "\n",
      " Grade Point Average : 1.6/2.7 \n",
      " July 2007 \n",
      " Associate Degree , Business , Major : The Arts \n",
      "\n",
      " Recent Course Work : \n",
      " - Intermediate Environmental Science ,   Physics ,   Marketing 1 ,   Electrical Engineering & Rhetoric ,   Graphic Design I ,   Biology ,   Ecology ,   21st Century Speech and Hearing Sciences\n",
      "\n",
      "[fake 3]\n",
      "State College of East Scott , Sep 2007   – Jul 2009 \n",
      " AA Psychology , Major in Logic \n",
      " Relevant Course Work : \n",
      " Comparative Literature 201 - Acting & Earth and Space Science - Elementary Neurobiology\n",
      "\n",
      "[fake 4]\n",
      "Prince College ,   March 2007    -   Sep 2013 \n",
      " Associate ; Computer Sciences \n",
      "\n",
      " College of Lake John   –   Gibbston , OR ,   Oct. 1981   - Current \n",
      " A.A.      Family and Consumer Science ,   Education Emphasis\n",
      "\n",
      "[fake 5]\n",
      "College of Port Anthonymouth   –   Dennisshire , NV \n",
      " Current GPA : 1.98/3.28 \n",
      " Associate Medicine\n",
      "\n",
      "[fake 6]\n",
      "North Dakota College ,   Rebeccamouth , Massachusetts \n",
      " Aug. 1988   - Present \n",
      " GPA : 1.76 / 3.12 \n",
      "\n",
      " Master of Fine Arts Anthropology \n",
      " Relevant Courses : \n",
      " - Informatics , 20th Century Communication * ( CXMA 2460 ) , Intermediate Nutrition Science , Music Composition 1 , \n",
      " Comparative Literature I\n",
      "\n",
      "[fake 7]\n",
      "Morris Community College ,   Lake Marissa , NJ \n",
      " Master 's ; Statistics \n",
      " Class of : June 1982   - Jan. 1992 \n",
      " Relevant Course Work- \n",
      " Contemporary Drama , Linguistics & Economics , Anthropology , 21st Century Music Education , Linguistics , Classics , Genetics 1\n",
      "\n",
      "[fake 8]\n",
      "Holmes College – Lake Joeltown , WA , April 1994   – March 2018 \n",
      " BCS ;   Performing Arts ,   Major , Archaeology\n",
      "\n",
      "[fake 9]\n",
      "A.S. Engineering and Technology \n",
      " Stephenson University     September 1978   – Jan 2011\n"
     ]
    }
   ],
   "source": [
    "fakes = list(generate_utils.generate_labeled_tokens(\n",
    "    education.generate.TEMPLATES,\n",
    "    education.generate.FIELDS,\n",
    "    n=10,\n",
    "    fixed_val_field_keys={\"ws\", \"fsep\", \"isep\"},\n",
    "))\n",
    "for i, fake in enumerate(fakes):\n",
    "    print(\"\\n[fake {}]\".format(i))\n",
    "    print(\" \".join(tok for tok, label in fake))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For good measure, the faked data can be \"augmented\" by adding in additional random variation that may affect both values and relationships between fields. This entails the use of an `augment_utils.Augmenter` class, which takes a set of transform functions and produces randomly modified versions of the original labeled tokens.\n",
    "\n",
    "Go ahead, run this next cell a few times in a row to see the additional variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[aug fake 0]\n",
      "Doctorate ,   Political Science , Minor , Journalism , Media Studies and Communication \n",
      " Community College of Vermont   –   Rileychester , NC \n",
      " Feb 2005 - Present\n",
      "\n",
      "[aug fake 1]\n",
      "Guerra University ,    Mendozaton , Oklahoma    Expected    Graduation : Mar 2015   – Current \n",
      " AA Culinary Arts \n",
      " Current GPA : 1.23\n",
      "\n",
      "[aug fake 2]\n",
      "Nguyen State \n",
      " Recent Courses- Environmental Studies and Policy ,   Ethnic and Gender Studies ,   Contemporary Resource Management \n",
      " Minor ; Natural Sciences \n",
      "\n",
      " Reese Polytechnic University \n",
      "\n",
      " Grade Point Average : 1.6/2.7 \n",
      " July 2007 \n",
      " Associate Degree , Business , Major : The Arts \n",
      "\n",
      " Recent Course Work : \n",
      " - Intermediate Environmental Science ,   Physics ,   Marketing 1 ,   Electrical Engineering & Rhetoric ,   Graphic Design I ,   Biology ,   Ecology ,   21st Century Speech and Hearing Sciences\n",
      "\n",
      "[aug fake 3]\n",
      "State College of East Scott , Sep 2007   – Jul 2009 \n",
      " AA Psychology , Major in Logic \n",
      " Relevant Course Work : \n",
      " Comparative Literature 201 - Acting & Earth and Space Science - Elementary Neurobiology\n",
      "\n",
      "[aug fake 4]\n",
      "Prince College ,   March 2007    -   Sep 2013 \n",
      " Associate ; Computer Sciences \n",
      "\n",
      " College of Lake John   –   Gibbston , OR ,   Oct. 1981   - Current \n",
      " A.A.      Family and Consumer Science ,   Education Emphasis\n",
      "\n",
      "[aug fake 5]\n",
      "College of Port Anthonymouth   –   Dennisshire , NV \n",
      " Current GPA : 1.98/3.28 \n",
      " Associate Medicine\n",
      "\n",
      "[aug fake 6]\n",
      "North Dakota College ,   Rebeccamouth , Massachusetts \n",
      " Aug. 1988   - Present \n",
      " GPA : 1.76 / 3.12 \n",
      "\n",
      " Master of Fine Arts Anthropology \n",
      " Relevant Courses : \n",
      " - Informatics , 20th Century Communication * ( CXMA 2460 ) , Intermediate Nutrition Science , Music Composition 1 , \n",
      " Comparative Literature I\n",
      "\n",
      "[aug fake 7]\n",
      "Morris Community College ,   Lake Marissa , NJ \n",
      " Master 's ; Statistics \n",
      " Class of : June 1982   - Jan. 1992 \n",
      " Relevant Course Work- \n",
      " Contemporary Drama , Linguistics & Economics , Anthropology , 21st Century Music Education , Linguistics , Classics , Genetics 1\n",
      "\n",
      "[aug fake 8]\n",
      "HOLMES COLLEGE – LAKE JOELTOWN , WA , April 1994   – March 2018 \n",
      " BCS ;   Performing Arts    ,   Major , Archaeology\n",
      "\n",
      "[aug fake 9]\n",
      "A.S. Engineering and Technology \n",
      " Stephenson University     September 1978   – Jan 2011\n"
     ]
    }
   ],
   "source": [
    "aug_fakes = [\n",
    "    education.augment.AUGMENTER.apply(tok_labels)\n",
    "    for tok_labels in fakes\n",
    "]\n",
    "for i, aug_fake in enumerate(aug_fakes):\n",
    "    print(\"\\n[aug fake {}]\".format(i))\n",
    "    print(\" \".join(tok for tok, label in aug_fake))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Next Steps?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should probably check out the API Reference in the docs. It's _a lot_, but should have the information you need to start using — and tinkering."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
