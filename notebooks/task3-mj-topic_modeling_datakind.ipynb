{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\n\nimport spacy\nimport pyLDAvis.gensim\npyLDAvis.enable_notebook()\n\nimport matplotlib.pyplot as plt\nimport umap\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# os.chdir(\"C:\\\\Users\\\\Mhamed\\\\Desktop\\\\datakind_dive\\\\msvdd_Bloc\\\\own_work\")\ndf = pd.read_csv('all_jobs.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"nlp = spacy.load('en_core_web_lg', disable=[\"tagger\", \"parser\", \"ner\"])\nnlp.max_length = 5000000\nraw_desc_texts = list(df.description.values)\nprocessed_desc_texts = [nlp(text) for text in raw_desc_texts]\nprocessed_desc_vectors = np.array([text.vector for text in processed_desc_texts])\n\nembedding = umap.UMAP().fit_transform(processed_desc_vectors)\nplt.scatter(embedding[:, 0], embedding[:, 1], s=0.1);","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from nltk.corpus import stopwords \nfrom nltk.stem.wordnet import WordNetLemmatizer\nimport string\n\nstop = stopwords.words('english')\nstop.extend(['degree', 'experience', 'work','field','related','from', 'subject', 're', 'edu',\n                   'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do',\n                   'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', \n                   'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', \n                   'line', 'even', 'also', 'may', 'take', 'come','li','br','datum','use','span','strong','tool','ul'])\n\nexclude = set(string.punctuation) \nlemma = WordNetLemmatizer()\ndef clean(doc):\n    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n    normalized = \" \".join(lemma.lemmatize(word) for word in stop_free.split())\n    return normalized\n\ndoc_complete = list(df.description)\ndata_words = [clean(doc).split() for doc in doc_complete]  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gensim\nfrom gensim.utils import lemmatize, simple_preprocess\nfrom gensim import corpora\n\nbigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\ntrigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \nbigram_mod = gensim.models.phrases.Phraser(bigram)\ntrigram_mod = gensim.models.phrases.Phraser(trigram)\n\ndef process_words(texts, stop_words=stop, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n    texts = [bigram_mod[doc] for doc in texts]\n    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n    texts_out = []\n    nlp = spacy.load('en', disable=['parser', 'ner'])\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    # remove stopwords once more after lemmatization\n    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n    return texts_out\n\ndata_ready = process_words(data_words)  # processed Text Data!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating the term dictionary of our courpus, where every unique term is assigned an index. \nid2word = corpora.Dictionary(data_ready)\n\n# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\ncorpus = [id2word.doc2bow(text) for text in data_ready]\n\n# Creating the object for LDA model using gensim library\nLda = gensim.models.ldamodel.LdaModel\n\n# Running and Trainign LDA model on the document term matrix.\nldamodel = Lda(corpus=corpus, id2word=id2word, num_topics=4, random_state=7,\n                                           alpha='symmetric',\n                                           iterations=100,\n                                           per_word_topics=True)\n\nprint(ldamodel.print_topics())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from bokeh.plotting import figure, output_file, show\nfrom bokeh.models import Label\nfrom bokeh.io import output_notebook\n\n# Get topic weights\ntopic_weights = []\nfor i, row_list in enumerate(ldamodel[corpus]):\n    topic_weights.append([w for i, w in row_list[0]])\n\n# Array of topic weights    \narr = pd.DataFrame(topic_weights).fillna(0).values\n# Dominant topic number in each doc\ntopic_num = np.argmax(arr, axis=1)\n\numap_lda = umap.UMAP().fit_transform(arr)\n\nimport seaborn as sns\nimport matplotlib.colors as mcolors\ncols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  \n\n# Plot the Topic Clusters using Bokeh\noutput_notebook()\nn_topics = 4\nmycolors = np.array([color for name, color in mcolors.TABLEAU_COLORS.items()])\nplot = figure(title=\"UMAP Clustering of {} LDA Topics\".format(n_topics), \n              plot_width=700, plot_height=500)\nplot.scatter(x=umap_lda[:,0], y=umap_lda [:,1], color=mycolors[topic_num])\nshow(plot)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.colors as mcolors\n\ncols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n\ncloud = WordCloud(stopwords=stop,\n                  background_color='white',\n                  width=2500,\n                  height=1800,\n                  max_words=10,\n                  colormap='tab10',\n                  color_func=lambda *args, **kwargs: cols[i],\n                  prefer_horizontal=1.0)\n\ntopics = ldamodel.show_topics(formatted=False)\n\nfig, axes = plt.subplots(2, 2, figsize=(10,10), sharex=True, sharey=True)\n\nfor i, ax in enumerate(axes.flatten()):\n    fig.add_subplot(ax)\n    topic_words = dict(topics[i][1])\n    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n    plt.gca().imshow(cloud)\n    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n    plt.gca().axis('off')\n\n\nplt.subplots_adjust(wspace=0, hspace=0)\nplt.axis('off')\nplt.margins(x=0, y=0)\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":1}