{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Brief: Résumé parsing and information extraction\n",
    "\n",
    "\n",
    "### CoNVO\n",
    "\n",
    "**Context:** Bloc is a career services management platform that builds smart career and data management tools for job-seekers and the organizations serving them. In particular, Bloc seeks to provide and facilitate access to tools for effectively presenting job-seekers' credentials and matching employers' job postings, and thereby improve outcomes.\n",
    "\n",
    "**Need:** Many job-seekers come to Bloc's platform with a résumé already written. Forcing new users to re-enter all that information before they can utilize other tools (e.g. for résumé evaluation) is tedious, at best. This can also reduce time pressure during in-person sessions facilitated by Bloc, where every second counts.\n",
    "\n",
    "**Vision:** Automated extraction of key information from existing résumés, submitted as PDFs while onboarding new users, in order to facilitate and streamline the process.\n",
    "\n",
    "**Outcome:** A standalone, proof-of-concept process for extracting key résumé information and returning it as structured data, complete with unit tests and documentation on expected usage, limitations, and potential improvements.\n",
    "\n",
    "\n",
    "### Data Summary\n",
    "\n",
    "Bloc has provided ~125 résumés with a variety of styles, layouts, and contents, in PDF format. Data quality seems good, and appears to be composed entirely of electronically-generated PDFs rather than (much more troublesome) scans of physical documents.\n",
    "\n",
    "They typically include personal contact information, professional experience, education, and skills; they sometimes include information on other relevant experience (volunteering, leadership, side projects), professional and academic associations, honors and awards, and personal interests; they rarely include a professional objective / statement of purpose and references.\n",
    "\n",
    "Since the amount of data is relatively small, and since résumés are so structured and standardized in terms of the information they include, a rules-based approach seems likely to succeed.\n",
    "\n",
    "\n",
    "### Proposed Methodology\n",
    "\n",
    "Cleanly extracting text from PDFs is tricky, since the format alters or throws out information for the sake of human-friendly layout, formatting, and such. Given this, it's best to use well-established tools for the text extraction, and highly accommodating parsing logic for the texts themselves. Rather than going full-bore on a complex, computer-vision based résumé parsing system, it'll be best to start with more foundational tools of text processing: regular expressions, fuzzy string matching, gazetteers/dictionaries, data sanitization/cleanup, and lots of trial-and-error.\n",
    "\n",
    "See the code below for something to get you started.\n",
    "\n",
    "\n",
    "### Definitions of Success\n",
    "\n",
    "- **Baseline:** A function that accepts a résumé (TBD: as filepath or already-extracted text) and returns structured data for the most common résumé components: contact information, professional experience, education, and skills. The quality of the extracted values may be messy or not fully parsed, but shouldn't contain values for other components. Atypical résumé components may be skipped. This function should have basic unit tests and documentation.\n",
    "- **Target:** A function that accepts a résumé and returns structured data for the most common résumé components (see Baseline), as well as other relevant experience, professional/academic associations, honors and awards, and personal interests. The quality of the extracted values should be almost fully parsed (e.g. no large blocks of relevant but unstructured text) and should not contain values for other components. Atypical résumé components may be skipped. This function should have unit tests covering a variety of expected scenarios and good documentation.\n",
    "- **Stretch:** A function that accepts a résumé and returns structured data for any component that could be reasonably expected in such a document. The quality of the extracted values should be almost fully parsed (see Target). Particularly unusual résumé components may be skipped. This function should have comprehensive unit tests and documentation.\n",
    "\n",
    "Note: We should try to get Bloc's buy-in / feedback on a schema, since they already ingest and store some of this data in their systems.\n",
    "\n",
    "\n",
    "### Risks\n",
    "\n",
    "It's possible that the information included in / extracted from résumés is too complex or varied for sufficiently accurate rules-based parsing, in which case a more sophisticated (ML- or DL-based) approach would be necessary, albeit impractical owing to time and data constraints. It's also possible that a rules-based approach is feasible, but too difficult / large a task for a single day's work.\n",
    "\n",
    "Another separate risk deals with personally-identifiable information (PII), which is intrinsic to a résumé, but which DataKind typically prefers to strip out of the data assigned to volunteers. A relatively practical solution would entail extracting text from the PDFs beforehand, then replacing direct PII (name and contact info) with placeholder values, but we'd still have volunteers working with indirect PII such as education / employment history. DataKind may not be able to abide such a middle ground.\n",
    "\n",
    "\n",
    "### DataDive Recommendation\n",
    "\n",
    "I think this is a good, interesting challenge for a DataDive. It's reasonably chunkable, and could be hacked on by one or multiple people simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import operator\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "import ftfy\n",
    "from faker import Faker\n",
    "from toolz import itertoolz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ftfy  5.6\n",
      "toolz 0.10.0\n",
      "re    2.2.1\n",
      "CPython 3.7.4\n",
      "IPython 7.8.0\n"
     ]
    }
   ],
   "source": [
    "%watermark -v -iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filepaths(dirpath, suffixes):\n",
    "    \"\"\"\n",
    "    Get full paths to all files under ``dirpath``\n",
    "    with a file type in ``suffixes``.\n",
    "\n",
    "    Args:\n",
    "        dirpath (:class:`pathlib.Path`)\n",
    "        suffixes (Set[str])\n",
    "    \n",
    "    Returns:\n",
    "        List[str]\n",
    "    \"\"\"\n",
    "    return sorted(\n",
    "        str(path) for path in dirpath.resolve().iterdir()\n",
    "        if path.is_file() and\n",
    "        path.suffix in suffixes\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# files: 128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/Users/burtondewilde/Desktop/datakind/bloc/raw_data/resumes/fellows/2018FellowsResumes[002-002].pdf',\n",
       " '/Users/burtondewilde/Desktop/datakind/bloc/raw_data/resumes/fellows/2018FellowsResumes[003-003].pdf',\n",
       " '/Users/burtondewilde/Desktop/datakind/bloc/raw_data/resumes/fellows/2018FellowsResumes[004-004].pdf']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_dir = pathlib.Path(\"/Users/burtondewilde/Desktop/datakind/bloc/raw_data/resumes/fellows\")\n",
    "filepaths = get_filepaths(raw_data_dir, {\".pdf\"})\n",
    "print(\"# files:\", len(filepaths))\n",
    "filepaths[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a caveat)\n",
    "\n",
    "Programmatically extracting text from a PDF with an atypical layout — such as a résumé — is _tricky_. Mistakes happen, and the results aren't always consistent with how a human would type it out.\n",
    "\n",
    "I tried several options (see below). The Python binding to Apache Tika (`python-tika`) seemed to give the nicest text extractions, although the JVM dependency is unfortunate. `textract` provides a convenient and consistent interface, but results are mediocre and installation involves a lot of extra packages. `pdfminer` and its many forks are highly customizable, but confusing to use and, to be honest, a hot mess as far as code quality goes. I'm surprised Python doesn't have a better solution to this problem, but _whatchagonnado_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_NAME = re.compile(r\"^(([(\\\"][A-Z]\\w+[)\\\"]|[A-Z]\\w+|[A-Z])[.,]?[ -]?){2,5}$\", flags=re.UNICODE)\n",
    "RE_URL = re.compile(\n",
    "    r\"(?:^|(?<![\\w/.]))\"\n",
    "    # protocol identifier\n",
    "    # r\"(?:(?:https?|ftp)://)\"  <-- alt?\n",
    "    r\"(?:(?:https?://|ftp://|www\\d{0,3}\\.))\"\n",
    "    # user:pass authentication\n",
    "    r\"(?:\\S+(?::\\S*)?@)?\"\n",
    "    r\"(?:\"\n",
    "    # IP address exclusion\n",
    "    # private & local networks\n",
    "    r\"(?!(?:10|127)(?:\\.\\d{1,3}){3})\"\n",
    "    r\"(?!(?:169\\.254|192\\.168)(?:\\.\\d{1,3}){2})\"\n",
    "    r\"(?!172\\.(?:1[6-9]|2\\d|3[0-1])(?:\\.\\d{1,3}){2})\"\n",
    "    # IP address dotted notation octets\n",
    "    # excludes loopback network 0.0.0.0\n",
    "    # excludes reserved space >= 224.0.0.0\n",
    "    # excludes network & broadcast addresses\n",
    "    # (first & last IP address of each class)\n",
    "    r\"(?:[1-9]\\d?|1\\d\\d|2[01]\\d|22[0-3])\"\n",
    "    r\"(?:\\.(?:1?\\d{1,2}|2[0-4]\\d|25[0-5])){2}\"\n",
    "    r\"(?:\\.(?:[1-9]\\d?|1\\d\\d|2[0-4]\\d|25[0-4]))\"\n",
    "    r\"|\"\n",
    "    # host name\n",
    "    r\"(?:(?:[a-z\\u00a1-\\uffff0-9]-?)*[a-z\\u00a1-\\uffff0-9]+)\"\n",
    "    # domain name\n",
    "    r\"(?:\\.(?:[a-z\\u00a1-\\uffff0-9]-?)*[a-z\\u00a1-\\uffff0-9]+)*\"\n",
    "    # TLD identifier\n",
    "    r\"(?:\\.(?:[a-z\\u00a1-\\uffff]{2,}))\"\n",
    "    r\")\"\n",
    "    # port number\n",
    "    r\"(?::\\d{2,5})?\"\n",
    "    # resource path\n",
    "    r\"(?:/\\S*)?\"\n",
    "    r\"(?:$|(?![\\w?!+&/]))\",\n",
    "    flags=re.UNICODE | re.IGNORECASE,\n",
    ")\n",
    "RE_SHORT_URL = re.compile(\n",
    "    r\"(?:^|(?<![\\w/.]))\"\n",
    "    # optional scheme\n",
    "    r\"(?:(?:https?://)?)\"\n",
    "    # domain\n",
    "    r\"(?:\\w-?)*?\\w+(?:\\.[a-z]{2,12}){1,3}\"\n",
    "    r\"/\"\n",
    "    # hash\n",
    "    r\"[^\\s.,?!'\\\"|+]{2,12}\"\n",
    "    r\"(?:$|(?![\\w?!+&/]))\",\n",
    "    flags=re.UNICODE | re.IGNORECASE,\n",
    ")\n",
    "RE_EMAIL = re.compile(\n",
    "    r\"(?:mailto:)?\"\n",
    "    r\"(?:^|(?<=[^\\w@.)]))([\\w+-](\\.(?!\\.))?)*?[\\w+-]@(?:\\w-?)*?\\w+(\\.([a-z]{2,})){1,3}\"\n",
    "    r\"(?:$|(?=\\b))\",\n",
    "    flags=re.UNICODE | re.IGNORECASE,\n",
    ")\n",
    "RE_PHONE_NUMBER = re.compile(\n",
    "    # core components of a phone number\n",
    "    r\"(?:^|(?<=[^\\w)]))(\\+?1[ .-]?)?(\\(?\\d{3}\\)?[ .-]?)?(\\d{3}[ .-]?\\d{4})\"\n",
    "    # extensions, etc.\n",
    "    r\"(\\s?(?:ext\\.?|[#x-])\\s?\\d{2,6})?(?:$|(?=\\W))\",\n",
    "    flags=re.UNICODE | re.IGNORECASE,\n",
    ")\n",
    "RE_STREET_ADDRESS = re.compile(\n",
    "    # r\"[ \\w]{3,}([A-Za-z]\\.)?([ \\w]*\\#\\d+)?,?[ \\w]{3,}, [A-Za-z]{2} \\d{5}(-\\d{4})?\",\n",
    "    r\"(\\d+ ((?! \\d+ ).)*?) [A-Za-z]{2} \\d{5}(-\\d{4})?\",\n",
    "    flags=re.UNICODE,\n",
    ")\n",
    "\n",
    "faker = Faker(local=\"en_US\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_clean_text(filepath, min_len=100):\n",
    "    \"\"\"\n",
    "    Extract text from a PDF at ``filepath`` using the first package\n",
    "    to get the job done in extracting at least ``min_len`` chars.\n",
    "    \n",
    "    Args:\n",
    "        filepath (str)\n",
    "        min_len (int)\n",
    "    \n",
    "    Returns:\n",
    "        str\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    funcs = [extract_text_tika, extract_text_pdfminer, extract_text_textract]\n",
    "    # extract text from pdf\n",
    "    for func in funcs:\n",
    "        _text = func(filepath)\n",
    "        if len(_text) >= min_len:\n",
    "            text = _text\n",
    "            break\n",
    "    if not text:\n",
    "        return text\n",
    "    else:\n",
    "        # correct any encoding / mojibake / other weirdness\n",
    "        return ftfy.fix_text(text)\n",
    "\n",
    "\n",
    "def replace_pii(text):\n",
    "    \"\"\"\n",
    "    Replace personally-identifying information in ``text``\n",
    "    with randomly generated fake equivalents.\n",
    "    \n",
    "    Args:\n",
    "        text (str)\n",
    "    \n",
    "    Returns:\n",
    "        str\n",
    "    \"\"\"\n",
    "    # let's start with names, which are usually on the first line    \n",
    "    first_line, *the_rest = text.split(\"\\n\", maxsplit=1)\n",
    "    first_line = RE_NAME.sub(faker.name(), first_line.strip())\n",
    "    text = \"\\n\".join([first_line] + the_rest)\n",
    "    # next, let's replace emails, urls, and addresses\n",
    "    # which are usually in the first \"chunk\" of info\n",
    "    # first_chunk, *the_rest = re.split(r\"\\n{2,}\", text, maxsplit=1)\n",
    "    first_chunk = text[:150]\n",
    "    the_rest = text[150:]\n",
    "    first_chunk = RE_PHONE_NUMBER.sub(faker.phone_number(), first_chunk)\n",
    "    first_chunk = RE_EMAIL.sub(faker.email(), first_chunk)\n",
    "    first_chunk = RE_URL.sub(faker.url(), first_chunk)\n",
    "    first_chunk = RE_STREET_ADDRESS.sub(faker.address().replace(\"\\n\", \" \"), first_chunk)\n",
    "    text = first_chunk + the_rest\n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_text_textract(filepath):\n",
    "    \"\"\"\n",
    "    Extract text from a PDF at ``filepath`` using ``textract`` + ``pdftotext``.\n",
    "    \n",
    "    Args:\n",
    "        filepath (str)\n",
    "    \n",
    "    Returns:\n",
    "        str\n",
    "    \"\"\"\n",
    "    # hiding the import so folks don't have to worry about installing it\n",
    "    import textract\n",
    "    \n",
    "    return textract.process(\n",
    "        filepath, method=\"pdftotext\", encoding=\"utf-8\"\n",
    "    ).decode(\"utf-8\").strip()\n",
    "\n",
    "\n",
    "def extract_text_pdfminer(filepath):\n",
    "    \"\"\"\n",
    "    Extract text from a PDF at ``filepath`` using ``yapdfminer``.\n",
    "\n",
    "    Args:\n",
    "        filepath (str)\n",
    "    \n",
    "    Returns:\n",
    "        str\n",
    "    \"\"\"\n",
    "    # hiding the import so folks don't have to worry about installing it\n",
    "    import pdfminer.converter\n",
    "    import pdfminer.layout\n",
    "    import pdfminer.pdfinterp\n",
    "    import pdfminer.pdfpage\n",
    "    \n",
    "    laparams = pdfminer.layout.LAParams(boxes_flow=0.5)\n",
    "    retstr = io.StringIO()\n",
    "    rsrcmgr = pdfminer.pdfinterp.PDFResourceManager()\n",
    "    device = pdfminer.converter.TextConverter(\n",
    "        rsrcmgr, retstr, codec=\"utf-8\", laparams=laparams)\n",
    "    interpreter = pdfminer.pdfinterp.PDFPageInterpreter(rsrcmgr, device)\n",
    "    fp = io.open(filepath, mode=\"rb\")\n",
    "    for page in pdfminer.pdfpage.PDFPage.get_pages(fp, set(), maxpages=0, caching=True, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "    text = retstr.getvalue()\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    retstr.close()\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def extract_text_tika(filepath):\n",
    "    \"\"\"\n",
    "    Extract text from a PDF at ``filepath`` using ``python-tika``.\n",
    "    \n",
    "    Args:\n",
    "        filepath (str)\n",
    "    \n",
    "    Returns:\n",
    "        str\n",
    "    \"\"\"\n",
    "    # hiding the import so folks don't have to worry about installing it\n",
    "    from tika import parser\n",
    "    \n",
    "    result = parser.from_file(filepath)\n",
    "    return result[\"content\"].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unable to extract text from /Users/burtondewilde/Desktop/datakind/bloc/raw_data/resumes/fellows/2018FellowsResumes[024-024].pdf\n",
      "unable to extract text from /Users/burtondewilde/Desktop/datakind/bloc/raw_data/resumes/fellows/2018FellowsResumes[027-027].pdf\n"
     ]
    }
   ],
   "source": [
    "out_data_dir = pathlib.Path(\"/Users/burtondewilde/Desktop/datakind/bloc/msvdd_Bloc/data/resumes\")\n",
    "for i, filepath in enumerate(filepaths):\n",
    "    text = extract_and_clean_text(filepath)\n",
    "    if not text:\n",
    "        print(\"unable to extract text from\", filepath)\n",
    "        continue\n",
    "\n",
    "    text = replace_pii(text)\n",
    "    fname = \"resume_{}.txt\".format(i)\n",
    "    with out_data_dir.joinpath(fname).open(mode=\"wt\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/burtondewilde/Desktop/datakind/bloc/msvdd_Bloc/data/resumes/resumes_data.zip')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save all resume texts as a zip archive\n",
    "archive_filepath = out_data_dir.joinpath(\"resumes_data.zip\")\n",
    "if archive_filepath.is_file():\n",
    "    os.remove(archive_filepath)\n",
    "temp_filepath = shutil.make_archive(\"resumes_data\", \"zip\", root_dir=out_data_dir)\n",
    "shutil.move(temp_filepath, archive_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
