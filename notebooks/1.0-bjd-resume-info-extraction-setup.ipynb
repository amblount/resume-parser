{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Résumé parsing and information extraction\n",
    "\n",
    "\n",
    "### CoNVO\n",
    "\n",
    "**Context:** Bloc is a career services management platform that builds smart career and data management tools for job-seekers and the organizations serving them. In particular, Bloc seeks to provide and facilitate access to tools for effectively presenting job-seekers' credentials and matching employers' job postings, and thereby improve outcomes.\n",
    "\n",
    "**Need:** Many job-seekers come to Bloc's platform with a résumé already written. Forcing new users to re-enter all that information before they can utilize other tools (e.g. for résumé evaluation) is tedious, at best. This can also reduce time pressure during in-person sessions facilitated by Bloc, where every second counts.\n",
    "\n",
    "**Vision:** Automated extraction of key information from existing résumés, submitted as PDFs while onboarding new users, in order to facilitate and streamline the process.\n",
    "\n",
    "**Outcome:** A standalone, proof-of-concept process for extracting key résumé information and returning it as structured data, complete with unit tests and documentation on expected usage, limitations, and potential improvements.\n",
    "\n",
    "\n",
    "### Data Summary\n",
    "\n",
    "Bloc has provided ~125 résumés with a variety of styles, layouts, and contents, in PDF format. (Bonus: ~2400 résumés scraped from external sources...) Data quality seems good, and appears to be composed entirely of electronically-generated PDFs rather than (much more troublesome) scans of physical documents.\n",
    "\n",
    "They typically include personal contact information, professional experience, education, and skills; they sometimes include information on other relevant experience (volunteering, leadership, side projects), professional and academic associations, honors and awards, and personal interests; they rarely include a professional objective / statement of purpose and references.\n",
    "\n",
    "Since the amount of data is relatively small, and since résumés are so structured and standardized in terms of the information they include, a rules-based approach seems likely to succeed.\n",
    "\n",
    "\n",
    "### Proposed Methodology\n",
    "\n",
    "Cleanly extracting text from PDFs is tricky, since the format alters or throws out information for the sake of human-friendly layout, formatting, and such. Given this, it's best to use well-established tools for the text extraction, and highly accommodating parsing logic for the texts themselves. Rather than going full-bore on a complex, computer-vision based résumé parsing system, it'll be best to start with more foundational tools of text processing: regular expressions, fuzzy string matching, gazetteers/dictionaries, data sanitization/cleanup, and lots of trial-and-error.\n",
    "\n",
    "See the code below for something to get you started.\n",
    "\n",
    "\n",
    "### Definitions of Success\n",
    "\n",
    "- **Baseline:** A function that accepts a résumé (TBD: as filepath or already-extracted text) and returns structured data for the most common résumé components: contact information, professional experience, education, and skills. The quality of the extracted values may be messy or not fully parsed, but shouldn't contain values for other components. Atypical résumé components may be skipped. This function should have basic unit tests and documentation.\n",
    "- **Target:** A function that accepts a résumé and returns structured data for the most common résumé components (see Baseline), as well as other relevant experience, professional/academic associations, honors and awards, and personal interests. The quality of the extracted values should be almost fully parsed (e.g. no large blocks of relevant but unstructured text) and should not contain values for other components. Atypical résumé components may be skipped. This function should have unit tests covering a variety of expected scenarios and good documentation.\n",
    "- **Stretch:** A function that accepts a résumé and returns structured data for any component that could be reasonably expected in such a document. The quality of the extracted values should be almost fully parsed (see Target). Particularly unusual résumé components may be skipped. This function should have comprehensive unit tests and documentation.\n",
    "\n",
    "Note: We should try to get Bloc's buy-in / feedback on a schema, since they already ingest and store some of this data in their systems.\n",
    "\n",
    "\n",
    "### Risks\n",
    "\n",
    "It's possible that the information included in / extracted from résumés is too complex or varied for sufficiently accurate rules-based parsing, in which case a more sophisticated (ML- or DL-based) approach would be necessary, albeit impractical owing to time and data constraints. It's also possible that a rules-based approach is feasible, but too difficult / large a task for a single day's work.\n",
    "\n",
    "Another separate risk deals with personally-identifiable information (PII), which is intrinsic to a résumé, but which DataKind typically prefers to strip out of the data assigned to volunteers. A relatively practical solution would entail extracting text from the PDFs beforehand, then replacing direct PII (name and contact info) with placeholder values, but we'd still have volunteers working with indirect PII such as education / employment history. DataKind may not be able to abide such a middle ground."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_NAME = re.compile(r\"^(([(\\\"][A-Z]\\w+[)\\\"]|[A-Z]\\w+|[A-Z])[.,]?[ -]?){2,5}$\", flags=re.UNICODE)\n",
    "RE_URL = re.compile(\n",
    "    r\"(?:^|(?<![\\w/.]))\"\n",
    "    # protocol identifier\n",
    "    # r\"(?:(?:https?|ftp)://)\"  <-- alt?\n",
    "    r\"(?:(?:https?://|ftp://|www\\d{0,3}\\.))\"\n",
    "    # user:pass authentication\n",
    "    r\"(?:\\S+(?::\\S*)?@)?\"\n",
    "    r\"(?:\"\n",
    "    # IP address exclusion\n",
    "    # private & local networks\n",
    "    r\"(?!(?:10|127)(?:\\.\\d{1,3}){3})\"\n",
    "    r\"(?!(?:169\\.254|192\\.168)(?:\\.\\d{1,3}){2})\"\n",
    "    r\"(?!172\\.(?:1[6-9]|2\\d|3[0-1])(?:\\.\\d{1,3}){2})\"\n",
    "    # IP address dotted notation octets\n",
    "    # excludes loopback network 0.0.0.0\n",
    "    # excludes reserved space >= 224.0.0.0\n",
    "    # excludes network & broadcast addresses\n",
    "    # (first & last IP address of each class)\n",
    "    r\"(?:[1-9]\\d?|1\\d\\d|2[01]\\d|22[0-3])\"\n",
    "    r\"(?:\\.(?:1?\\d{1,2}|2[0-4]\\d|25[0-5])){2}\"\n",
    "    r\"(?:\\.(?:[1-9]\\d?|1\\d\\d|2[0-4]\\d|25[0-4]))\"\n",
    "    r\"|\"\n",
    "    # host name\n",
    "    r\"(?:(?:[a-z\\u00a1-\\uffff0-9]-?)*[a-z\\u00a1-\\uffff0-9]+)\"\n",
    "    # domain name\n",
    "    r\"(?:\\.(?:[a-z\\u00a1-\\uffff0-9]-?)*[a-z\\u00a1-\\uffff0-9]+)*\"\n",
    "    # TLD identifier\n",
    "    r\"(?:\\.(?:[a-z\\u00a1-\\uffff]{2,}))\"\n",
    "    r\")\"\n",
    "    # port number\n",
    "    r\"(?::\\d{2,5})?\"\n",
    "    # resource path\n",
    "    r\"(?:/\\S*)?\"\n",
    "    r\"(?:$|(?![\\w?!+&/]))\",\n",
    "    flags=re.UNICODE | re.IGNORECASE,\n",
    ")\n",
    "RE_SHORT_URL = re.compile(\n",
    "    r\"(?:^|(?<![\\w/.]))\"\n",
    "    # optional scheme\n",
    "    r\"(?:(?:https?://)?)\"\n",
    "    # domain\n",
    "    r\"(?:\\w-?)*?\\w+(?:\\.[a-z]{2,12}){1,3}\"\n",
    "    r\"/\"\n",
    "    # hash\n",
    "    r\"[^\\s.,?!'\\\"|+]{2,12}\"\n",
    "    r\"(?:$|(?![\\w?!+&/]))\",\n",
    "    flags=re.UNICODE | re.IGNORECASE,\n",
    ")\n",
    "RE_EMAIL = re.compile(\n",
    "    r\"(?:mailto:)?\"\n",
    "    r\"(?:^|(?<=[^\\w@.)]))([\\w+-](\\.(?!\\.))?)*?[\\w+-]@(?:\\w-?)*?\\w+(\\.([a-z]{2,})){1,3}\"\n",
    "    r\"(?:$|(?=\\b))\",\n",
    "    flags=re.UNICODE | re.IGNORECASE,\n",
    ")\n",
    "RE_PHONE_NUMBER = re.compile(\n",
    "    # core components of a phone number\n",
    "    r\"(?:^|(?<=[^\\w)]))(\\+?1[ .-]?)?(\\(?\\d{3}\\)?[ .-]?)?(\\d{3}[ .-]?\\d{4})\"\n",
    "    # extensions, etc.\n",
    "    r\"(\\s?(?:ext\\.?|[#x-])\\s?\\d{2,6})?(?:$|(?=\\W))\",\n",
    "    flags=re.UNICODE | re.IGNORECASE,\n",
    ")\n",
    "RE_STREET_ADDRESS = re.compile(\n",
    "    # r\"[ \\w]{3,}([A-Za-z]\\.)?([ \\w]*\\#\\d+)?,?[ \\w]{3,}, [A-Za-z]{2} \\d{5}(-\\d{4})?\",\n",
    "    r\"(\\d+ ((?! \\d+ ).)*?) [A-Za-z]{2} \\d{5}(-\\d{4})?\",\n",
    "    flags=re.UNICODE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filepaths(dirpath, suffixes):\n",
    "    \"\"\"\n",
    "    Get full paths to all files under ``dirpath``\n",
    "    with a file type in ``suffixes``.\n",
    "\n",
    "    Args:\n",
    "        dirpath (:class:`pathlib.Path`)\n",
    "        suffixes (Set[str])\n",
    "    \n",
    "    Returns:\n",
    "        List[str]\n",
    "    \"\"\"\n",
    "    return sorted(\n",
    "        str(path) for path in dirpath.resolve().iterdir()\n",
    "        if path.is_file() and\n",
    "        path.suffix in suffixes\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Generation\n",
    "\n",
    "**Note:** Don't run this section! (Besides, you _can't_, because the raw data has not been made available to you.) This is just to help you understand the data's provenance. Instead, download the already-generated datasets from OneDrive. (There's a link on the Bloc Project Home document in Dropbox.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import operator\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "import ftfy\n",
    "from faker import Faker\n",
    "from toolz import itertoolz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ftfy  5.6\n",
      "re    2.2.1\n",
      "toolz 0.10.0\n",
      "CPython 3.7.4\n",
      "IPython 7.8.0\n"
     ]
    }
   ],
   "source": [
    "%watermark -v -iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAKER = Faker(local=\"en_US\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dirpath = pathlib.Path(\"/Users/burtondewilde/Desktop/datakind/bloc/msvdd_Bloc/data/resumes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_pii(text):\n",
    "    \"\"\"\n",
    "    Replace personally-identifying information in ``text``\n",
    "    with randomly generated fake equivalents.\n",
    "    \n",
    "    Args:\n",
    "        text (str)\n",
    "    \n",
    "    Returns:\n",
    "        str\n",
    "    \"\"\"\n",
    "    # let's start with names, which are usually on the first line    \n",
    "    first_line, *the_rest = text.split(\"\\n\", maxsplit=1)\n",
    "    first_line = RE_NAME.sub(FAKER.name(), first_line.strip())\n",
    "    text = \"\\n\".join([first_line] + the_rest)\n",
    "    # next, let's replace emails, urls, and addresses\n",
    "    # which are usually in the first \"chunk\" of info\n",
    "    # first_chunk, *the_rest = re.split(r\"\\n{2,}\", text, maxsplit=1)\n",
    "    first_chunk = text[:150]\n",
    "    the_rest = text[150:]\n",
    "    first_chunk = RE_PHONE_NUMBER.sub(FAKER.phone_number(), first_chunk)\n",
    "    first_chunk = RE_EMAIL.sub(FAKER.email(), first_chunk)\n",
    "    first_chunk = RE_URL.sub(FAKER.url(), first_chunk)\n",
    "    first_chunk = RE_STREET_ADDRESS.sub(FAKER.address().replace(\"\\n\", \" \"), first_chunk)\n",
    "    text = first_chunk + the_rest\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bloc Fellows' résumés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# files: 128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/Users/burtondewilde/Desktop/datakind/bloc/raw_data/resumes/fellows/2018FellowsResumes[002-002].pdf',\n",
       " '/Users/burtondewilde/Desktop/datakind/bloc/raw_data/resumes/fellows/2018FellowsResumes[003-003].pdf',\n",
       " '/Users/burtondewilde/Desktop/datakind/bloc/raw_data/resumes/fellows/2018FellowsResumes[004-004].pdf']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_fellows_dirpath = pathlib.Path(\"/Users/burtondewilde/Desktop/datakind/bloc/raw_data/resumes/fellows\")\n",
    "filepaths = get_filepaths(in_fellows_dirpath, {\".pdf\"})\n",
    "print(\"# files:\", len(filepaths))\n",
    "filepaths[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a caveat)\n",
    "\n",
    "Programmatically extracting text from a PDF with an atypical layout — such as a résumé — is _tricky_. Mistakes happen, and the results aren't always consistent with how a human would type it out.\n",
    "\n",
    "I tried several options (see below). The Python binding to Apache Tika (`python-tika`) seemed to give the nicest text extractions, although the JVM dependency is unfortunate. `textract` provides a convenient and consistent interface, but results are mediocre and installation involves a lot of extra packages. `pdfminer` and its many forks are highly customizable, but confusing to use and, to be honest, a hot mess as far as code quality goes. I'm surprised Python doesn't have a better solution to this problem, but _whatchagonnado_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(filepath, *, min_len):\n",
    "    \"\"\"\n",
    "    Extract text from a PDF at ``filepath`` using the first package\n",
    "    to get the job done in extracting at least ``min_len`` chars.\n",
    "    \n",
    "    Args:\n",
    "        filepath (str)\n",
    "        min_len (int)\n",
    "    \n",
    "    Returns:\n",
    "        str\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    funcs = [extract_text_tika, extract_text_pdfminer, extract_text_textract]\n",
    "    for func in funcs:\n",
    "        _text = func(filepath)\n",
    "        if len(_text) >= min_len:\n",
    "            text = _text\n",
    "            break\n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_text_textract(filepath):\n",
    "    \"\"\"\n",
    "    Extract text from a PDF at ``filepath`` using ``textract`` + ``pdftotext``.\n",
    "    \n",
    "    Args:\n",
    "        filepath (str)\n",
    "    \n",
    "    Returns:\n",
    "        str\n",
    "    \"\"\"\n",
    "    # hiding the import so folks don't have to worry about installing it\n",
    "    import textract\n",
    "    \n",
    "    return textract.process(\n",
    "        filepath, method=\"pdftotext\", encoding=\"utf-8\"\n",
    "    ).decode(\"utf-8\").strip()\n",
    "\n",
    "\n",
    "def extract_text_pdfminer(filepath):\n",
    "    \"\"\"\n",
    "    Extract text from a PDF at ``filepath`` using ``yapdfminer``.\n",
    "\n",
    "    Args:\n",
    "        filepath (str)\n",
    "    \n",
    "    Returns:\n",
    "        str\n",
    "    \"\"\"\n",
    "    # hiding the import so folks don't have to worry about installing it\n",
    "    import pdfminer.converter\n",
    "    import pdfminer.layout\n",
    "    import pdfminer.pdfinterp\n",
    "    import pdfminer.pdfpage\n",
    "    \n",
    "    laparams = pdfminer.layout.LAParams(boxes_flow=0.5)\n",
    "    retstr = io.StringIO()\n",
    "    rsrcmgr = pdfminer.pdfinterp.PDFResourceManager()\n",
    "    device = pdfminer.converter.TextConverter(\n",
    "        rsrcmgr, retstr, codec=\"utf-8\", laparams=laparams)\n",
    "    interpreter = pdfminer.pdfinterp.PDFPageInterpreter(rsrcmgr, device)\n",
    "    fp = io.open(filepath, mode=\"rb\")\n",
    "    for page in pdfminer.pdfpage.PDFPage.get_pages(fp, set(), maxpages=0, caching=True, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "    text = retstr.getvalue()\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    retstr.close()\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def extract_text_tika(filepath):\n",
    "    \"\"\"\n",
    "    Extract text from a PDF at ``filepath`` using ``python-tika``.\n",
    "    \n",
    "    Args:\n",
    "        filepath (str)\n",
    "    \n",
    "    Returns:\n",
    "        str\n",
    "    \"\"\"\n",
    "    # hiding the import so folks don't have to worry about installing it\n",
    "    from tika import parser\n",
    "    \n",
    "    result = parser.from_file(filepath)\n",
    "    return result[\"content\"].strip()\n",
    "\n",
    "\n",
    "def clean_fellows_text(text):\n",
    "    \"\"\"\n",
    "    Correct any encoding / mojibake / unicode weirdness, and remove\n",
    "    superfluous numbering at the end of the résumé.\n",
    "    \n",
    "    Args:\n",
    "        text (str)\n",
    "    \n",
    "    Returns:\n",
    "        str\n",
    "    \"\"\"\n",
    "    text = ftfy.fix_text(text)\n",
    "    text = re.sub(r\"\\s+\\d+\\s*?$\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unable to extract text from /Users/burtondewilde/Desktop/datakind/bloc/raw_data/resumes/fellows/2018FellowsResumes[024-024].pdf\n",
      "unable to extract text from /Users/burtondewilde/Desktop/datakind/bloc/raw_data/resumes/fellows/2018FellowsResumes[027-027].pdf\n"
     ]
    }
   ],
   "source": [
    "out_fellows_fpath = out_dirpath.joinpath(\"fellows_resumes.zip\")\n",
    "with zipfile.ZipFile(out_fellows_fpath, mode=\"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "    for i, filepath in enumerate(filepaths):\n",
    "        text = extract_text_from_pdf(filepath, min_len=150)\n",
    "        if not text:\n",
    "            print(\"unable to extract text from\", filepath)\n",
    "            continue\n",
    "        text = clean_fellows_text(text)\n",
    "        text = replace_pii(text)\n",
    "        fname = \"fellows_resume_{}.txt\".format(i)\n",
    "        zf.writestr(fname, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Amina's bonus scraped résumés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# files: 2432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/Users/burtondewilde/Desktop/datakind/bloc/raw_data/resumes/bonus/_resume_ab0iv8_hopewell-fluorescence-stainless-steel-haledon-nj.txt',\n",
       " '/Users/burtondewilde/Desktop/datakind/bloc/raw_data/resumes/bonus/_resume_ab2abk_available-upon-request-givers-cmo-icd-new-york-ny.txt',\n",
       " '/Users/burtondewilde/Desktop/datakind/bloc/raw_data/resumes/bonus/_resume_ab3i5x_july-2013-5th-pharm-upon-request-harrison-nj-07029.txt']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_bonus_dirpath = pathlib.Path(\"/Users/burtondewilde/Desktop/datakind/bloc/raw_data/resumes/bonus\")\n",
    "filepaths = get_filepaths(in_bonus_dirpath, {\".txt\"})\n",
    "print(\"# files:\", len(filepaths))\n",
    "filepaths[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a caveat)\n",
    "\n",
    "ADD CAVEAT HERE. (There is one.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_bonus_text(text):\n",
    "    \"\"\"\n",
    "    Correct any encoding / mojibake / unicode weirdness, and remove\n",
    "    superfluous \"resumes (in)\" lines at the end of the résumé.\n",
    "    \n",
    "    Args:\n",
    "        text (str)\n",
    "    \n",
    "    Returns:\n",
    "        str\n",
    "    \"\"\"\n",
    "    text = ftfy.fix_text(text)\n",
    "    text = re.sub(r\"([\\w]+ ?){1,3}resumes( in ([\\w,]+ ?){1,3})?\\n\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_bonus_fpath = out_dirpath.joinpath(\"bonus_resumes.zip\")\n",
    "with zipfile.ZipFile(out_bonus_fpath, mode=\"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "    for i, filepath in enumerate(filepaths):\n",
    "        with io.open(filepath, mode=\"r\") as f:\n",
    "            text = f.read()\n",
    "        if not text:\n",
    "            print(\"unable to extract text from\", filepath)\n",
    "            continue\n",
    "        text = clean_bonus_text(text)\n",
    "        text = replace_pii(text)\n",
    "        fname = \"bonus_resume_{}.txt\".format(i)\n",
    "        zf.writestr(fname, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import io\n",
    "import operator\n",
    "import pathlib\n",
    "\n",
    "from toolz import itertoolz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ftfy  5.6\n",
      "re    2.2.1\n",
      "toolz 0.10.0\n",
      "CPython 3.7.4\n",
      "IPython 7.8.0\n"
     ]
    }
   ],
   "source": [
    "%watermark -v -iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_BULLETS = re.compile(r\"[\\u25cf\\u2022\\u2023\\u2043]\", flags=re.UNICODE)\n",
    "RE_BREAKING_SPACE = re.compile(r\"(\\r\\n|[\\n\\v]){2,}\", flags=re.UNICODE)\n",
    "RE_NONBREAKING_SPACE = re.compile(r\"[^\\S\\n\\v]+\", flags=re.UNICODE)\n",
    "RE_MONTH = re.compile(\n",
    "    r\"(jan|january|feb|february|mar|march|apr|april|may|jun|june|jul|july|aug|august|sep|september|oct|october|nov|november|dec|december)\",\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "RE_YEAR = re.compile(r\"((19|20)\\d{2})\")\n",
    "\n",
    "SECTION_HEADERS = {\n",
    "    \"education\": {\n",
    "        \"education\",\n",
    "    },\n",
    "    \"experience\": {\n",
    "        \"experience\",\n",
    "        \"work experience\",\n",
    "        \"professional experience\",\n",
    "        \"work & research experience\",\n",
    "        \"relevant experience\",\n",
    "        \"experiences\",\n",
    "        \"additional experience\",\n",
    "        \"leadership\",\n",
    "        \"leadership experience\",\n",
    "        \"leadership and service\",\n",
    "    },\n",
    "    \"skills\": {\n",
    "        \"skills\",\n",
    "        \"technical skills\",\n",
    "        \"skills & expertise\",\n",
    "        \"technological skills\",\n",
    "        \"tools\",\n",
    "        \"languages\",\n",
    "        \"programming languages\",\n",
    "        \"languages and technologies\",\n",
    "        \"language and technologies\",\n",
    "    },\n",
    "    \"achievements\": {\n",
    "        \"achievements\",\n",
    "        \"awards\",\n",
    "        \"honors\",\n",
    "        \"honors & awards\",\n",
    "        \"honors, awards, and memberships\",\n",
    "        \"fellowships & awards\",\n",
    "        \"awards and certifications\",\n",
    "    },\n",
    "    \"projects\": {\n",
    "        \"projects\",\n",
    "        \"side projects\",\n",
    "        \"technical projects\",\n",
    "        \"programming projects\",\n",
    "        \"github projects\",\n",
    "        \"other projects\",\n",
    "    },\n",
    "    \"activities\": {\n",
    "        \"activities\",\n",
    "        \"volunteering\",\n",
    "        \"activities and student groups\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_resume_text(filepath):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        filepath (str)\n",
    "    \n",
    "    Returns:\n",
    "        str\n",
    "    \"\"\"\n",
    "    with io.open(filepath, mode=\"rt\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocess_resume_text(text):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        text (str)\n",
    "        \n",
    "    Returns:\n",
    "        str\n",
    "    \"\"\"\n",
    "    # clean up weird stuff\n",
    "    text = RE_BULLETS.sub(\"-\", text)\n",
    "    # normalize whitespace\n",
    "    text = RE_NONBREAKING_SPACE.sub(\" \", text).strip()\n",
    "    text = RE_BREAKING_SPACE.sub(r\"\\n\\n\", text)\n",
    "    # TODO: any other roughness that can be consistently smoothed out\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_section_idxs(lines):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        lines (List[str])\n",
    "    \n",
    "    Returns:\n",
    "        List[Tuple[str, int]]\n",
    "    \"\"\"\n",
    "    section_idxs = [(\"START\", 0)]\n",
    "    for idx, line in enumerate(lines):\n",
    "        for section, headers in SECTION_HEADERS.items():\n",
    "            if (\n",
    "                any(line.lower() == header for header in headers) or\n",
    "                any(line.lower().startswith(header + \":\") for header in headers)\n",
    "            ):\n",
    "                section_idxs.append((section, idx))\n",
    "    section_idxs.append((\"END\", len(lines)))\n",
    "    return section_idxs\n",
    "\n",
    "\n",
    "def get_section_lines(lines, section_idxs):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        lines (List[str])\n",
    "        section_idxs (List[Tuple[str, int]])\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, List[str]]\n",
    "    \"\"\"\n",
    "    section_lines = collections.defaultdict(list)\n",
    "    for (section, idx1), (_, idx2) in itertoolz.sliding_window(2, section_idxs):\n",
    "        section_lines[section].extend(lines[idx1 : idx2])\n",
    "    return dict(section_lines)\n",
    "\n",
    "\n",
    "def parse_skills_section(lines):\n",
    "    \"\"\"\n",
    "    Super rough example for extracting structured data from skills...\n",
    "    \n",
    "    Args:\n",
    "        lines (List[str])\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, List[str]]\n",
    "    \"\"\"\n",
    "    skills = [\n",
    "        skill.lstrip(\"- \")\n",
    "        for line in lines\n",
    "        for skill in re.split(r\", +\", line)\n",
    "        if skill.strip() and\n",
    "        skill.strip().lower() not in SECTION_HEADERS[\"skills\"]\n",
    "    ]\n",
    "    return {\"skills\": skills}\n",
    "\n",
    "\n",
    "def parse_education_section(lines):\n",
    "    raise NotImplementedError(\"Greetings, DataDiver!\")\n",
    "    \n",
    "\n",
    "# and so on and so forth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# files: 126\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/Users/burtondewilde/Desktop/datakind/bloc/msvdd_Bloc/data/resumes/resume_0.txt',\n",
       " '/Users/burtondewilde/Desktop/datakind/bloc/msvdd_Bloc/data/resumes/resume_1.txt',\n",
       " '/Users/burtondewilde/Desktop/datakind/bloc/msvdd_Bloc/data/resumes/resume_10.txt']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = pathlib.Path(\"/Users/burtondewilde/Desktop/datakind/bloc/msvdd_Bloc/data/resumes\")\n",
    "filepaths = get_filepaths(data_dir, {\".txt\"})\n",
    "print(\"# files:\", len(filepaths))\n",
    "filepaths[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filepath in filepaths:\n",
    "    text = load_resume_text(filepath)\n",
    "    text = preprocess_resume_text(text)\n",
    "    lines = [line.strip() for line in text.split(\"\\n\")]\n",
    "    section_idxs = get_section_idxs(lines)\n",
    "    section_lines = get_section_lines(lines, section_idxs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SKILLS',\n",
       " '',\n",
       " '- Java, Python, C/C++, C#, Git, MySQL',\n",
       " '- HTML, CSS, JavaScript, PHP, Sass, LESS',\n",
       " '',\n",
       " '- React, Angular, NodeJS',\n",
       " '- AWS, Drupal, WordPress']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "section_lines.get(\"skills\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'skills': ['Java',\n",
       "  'Python',\n",
       "  'C/C++',\n",
       "  'C#',\n",
       "  'Git',\n",
       "  'MySQL',\n",
       "  'HTML',\n",
       "  'CSS',\n",
       "  'JavaScript',\n",
       "  'PHP',\n",
       "  'Sass',\n",
       "  'LESS',\n",
       "  'React',\n",
       "  'Angular',\n",
       "  'NodeJS',\n",
       "  'AWS',\n",
       "  'Drupal',\n",
       "  'WordPress']}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_skills_section(section_lines.get(\"skills\", []))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_just curious_: what are the most common section headers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('education', 86),\n",
       " ('projects', 45),\n",
       " ('skills', 44),\n",
       " ('experience', 41),\n",
       " ('work experience', 26),\n",
       " ('technical skills', 21),\n",
       " ('awards', 17),\n",
       " ('languages', 10),\n",
       " ('leadership', 9),\n",
       " ('professional experience', 8),\n",
       " ('achievements', 4),\n",
       " ('activities', 4),\n",
       " ('education:', 4),\n",
       " ('relevant experience', 4),\n",
       " ('leadership experience', 3),\n",
       " ('experience:', 3),\n",
       " ('volunteering', 2),\n",
       " ('experiences', 2),\n",
       " ('honors & awards', 2),\n",
       " ('additional experience', 2),\n",
       " ('languages and technologies', 2),\n",
       " ('programming projects', 2),\n",
       " ('projects:', 2),\n",
       " ('tools', 2),\n",
       " ('1', 2),\n",
       " ('side projects', 2)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header_counts = collections.Counter()\n",
    "for filepath in filepaths:\n",
    "    text = preprocess_resume_text(load_resume_text(filepath))\n",
    "    lines = [line.strip() for line in text.split(\"\\n\")]\n",
    "    section_idxs = get_section_idxs(lines)\n",
    "    header_counts.update(lines[idx].lower() for _, idx in section_idxs if idx != len(lines))\n",
    "[item for item in header_counts.most_common() if item[1] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
